{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate bitsandbytes sentencepiece\n",
    "!git clone https://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@github.com/Alintermans/Thesis.git\n",
    "!pip install git+https://github.com/Alintermans/transformers.git\n",
    "%cd Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from SongUtils import divide_song_into_paragraphs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, StoppingCriteriaList, MaxLengthCriteria, LogitsProcessorList, NoBadWordsLogitsProcessor, NoRepeatNGramLogitsProcessor, BitsAndBytesConfig\n",
    "from BeamSearchScorerConstrained import BeamSearchScorerConstrained\n",
    "from Constraint import ConstraintList, Constraint\n",
    "from SyllableConstraint import SyllableConstraint\n",
    "from SongUtils import get_syllable_count_of_sentence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token= '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',)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\n",
    "                                                 #torch_dtype=torch.bfloat16, \n",
    "                                                 #low_cpu_mem_usage=True, \n",
    "                                                 token= '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',\n",
    "                                                 quantization_config=bnb_config,\n",
    "                                                 # max_memory={\"cpu\": \"11GIB\"},\n",
    "                                                 # offload_state_dict=True,\n",
    "                                                 # offload_folder = '/Volumes/Samsung\\ SSD/offload'\n",
    "                                                 #load_in_8bit=True\n",
    "                                                 )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "initial_text_prompt = \"You're a parodie genrator that will write beatifull parodies and make sure that the syllable count of the parodie is the same as the original song\\n\"\n",
    "\n",
    "context = \"The new parodie will be about that pineaple shouldn't be on pizza\\n\"\n",
    "\n",
    "original_song = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "num_beams = 2\n",
    "\n",
    "forbidden_charachters = ['[', ']', '(', ')', '{', '}', '<', '>', '|', '\\\\', '/', '_', '——', ' — ', '..' '+', '=', '*', '&', '^', '%', '$', '#', '@', '!', '~', '`', ';', ':', '\"', \"'\", ',', '.', '?', '\\n', '...']\n",
    "forbidden_tokens = [[tokenizer.encode(c)[0]] for c in forbidden_charachters]\n",
    "\n",
    "def generate_parodie_line(prompt, line):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    syllable_amount_prompt = get_syllable_count_of_sentence(prompt)\n",
    "    syllable_amount_line = get_syllable_count_of_sentence(line)\n",
    "    print('Syllable count prompt: ', syllable_amount_prompt)\n",
    "    print(\"Line: \", line, '| Syllable count: ', syllable_amount_line)\n",
    "    syllable_amount = syllable_amount_prompt + syllable_amount_line\n",
    "    constraints = ConstraintList([SyllableConstraint(syllable_amount, tokenizer)])\n",
    "    stopping_criteria_list = constraints.get_stopping_criteria_list()\n",
    "    stopping_criteria = StoppingCriteriaList(stopping_criteria_list)\n",
    "    logits_processor_list = constraints.get_logits_processor_list()\n",
    "    logits_processor_list.append(NoRepeatNGramLogitsProcessor(2))\n",
    "    logits_processor_list.append(NoBadWordsLogitsProcessor(forbidden_tokens, eos_token_id=tokenizer.eos_token_id))\n",
    "    logits_processor = LogitsProcessorList(logits_processor_list)\n",
    "\n",
    "    beam_scorer = BeamSearchScorerConstrained(\n",
    "        batch_size= model_inputs['input_ids'].shape[0],\n",
    "        max_length=1000,\n",
    "        num_beams=num_beams,\n",
    "        device=model.device,\n",
    "        constraints = constraints,\n",
    "    )\n",
    "\n",
    "    generated = model.beam_search(\n",
    "        torch.cat([model_inputs['input_ids']] * num_beams),\n",
    "        beam_scorer,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        logits_processor = logits_processor,\n",
    "        )\n",
    "\n",
    "    sentence = tokenizer.decode(generated[0], skip_special_tokens=True)[len(prompt):]\n",
    "    print('syllable count: ', get_syllable_count_of_sentence(sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "original_song_file_path = 'Experiments/GPT-2Constraints/songs/it_is_over_now-taylor_swift_small.txt'\n",
    "original_song_file = open(original_song_file_path, 'r')\n",
    "original_song += original_song_file.read()\n",
    "original_song_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "paragraps = divide_song_into_paragraphs(original_song)\n",
    "\n",
    "original_song = \"ORIGINAL SONG: \\n\\n\" + original_song \n",
    "\n",
    "parodie = \"\\n\\nAlready generated PARODIE: \\n\\n\"\n",
    "next_line_text = \"The original line is: \"\n",
    "next_line_text_parodie = \"The parodie line is: \"\n",
    "\n",
    "prompt = initial_text_prompt + context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# generate line per line\n",
    "for paragraph in paragraps:\n",
    "    parodie += paragraph[0] + \"\\n\"\n",
    "    prompt += paragraph[0] + \"\\n\"\n",
    "    for line in paragraph[1]:\n",
    "        new_prompt = prompt + parodie + next_line_text + line + \"\\n\" + next_line_text_parodie\n",
    "        result = generate_parodie_line(new_prompt, line) + \"\\n\"\n",
    "        parodie += result\n",
    "        \n",
    "        print(line, \" | \",result)\n",
    "    parodie += \"\\n\"\n",
    "    prompt += \"\\n\"\n",
    "    \n",
    "\n",
    "print(\"Parodie: \", parodie)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
