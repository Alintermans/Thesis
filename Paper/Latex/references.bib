@misc{Grammarly,
  title = {{Grammarly}},
  howpublished = {\url{https://www.grammarly.com}},
  organization = {Grammarly Inc.}
}


@inproceedings{devlin_bert_2019,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {{arXiv}:1810.04805},
	publisher = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2024-01-05},
        year = {2019},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/393HCTPD/1810.html:text/html;Full Text PDF:/Users/antonlintermans/Zotero/storage/XDS8IIYJ/Devlin e.a. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{manning-et-al-2014-corenlp,
    title = {The Stanford CoreNLP Natural Language Processing Toolkit},
    author = {Manning, Christopher D. and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven J. and McClosky, David},
    booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
    pages = {55--60},
    year = {2014},
    url = {https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf}
}



@inproceedings{T5_raffel_exploring_2023,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	number = {{arXiv}:1910.10683},
	publisher = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	urldate = {2024-01-04},
	year = {2020},
	eprinttype = {arxiv},
	eprint = {1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/2P2ISKVY/1910.html:text/html;Full Text PDF:/Users/antonlintermans/Zotero/storage/CMBYBKMB/Raffel e.a. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}


@inproceedings{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {https://arxiv.org/abs/1301.3781v3},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	titleaddon = {{arXiv}.org},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2024-01-05},
	date = {2013-01-16},
        year = {2013},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/PEAWR2NB/Mikolov e.a. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf},
}

@inproceedings{pennington_glove_2014,
	location = {Doha, Qatar},
	title = {{GloVe}: Global Vectors for Word Representation},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	shorttitle = {{GloVe}},
	eventtitle = {{EMNLP} 2014},
        year = {2014},
	pages = {1532--1543},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	urldate = {2024-01-05},
	date = {2014-10},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/RZKPY63I/Pennington e.a. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}

@misc{riedl_weirdai_Github,
  author = {Riedl, Mark},
  title = {weirdai},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/markriedl/weirdai}},
  commit = {The latest commit SHA}
}

@misc{ChatGPT4,
  title = {ChatGPT-4},
  author = {{OpenAI}},
  howpublished = {\url{https://www.chat.openai.com}},
  year = {2023},
}

@misc{ChatGPT35Turbo,
  title = {ChatGPT-3.5 Turbo},
  author = {{OpenAI}},
  howpublished = {\url{https://www.chat.openai.com}},
  year = {2022},
}


@paper{gpt-2,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	langid = {english},
        year = {2018},
	file = {Radford e.a. - Language Models are Unsupervised Multitask Learner.pdf:/Users/antonlintermans/Zotero/storage/8ZWAA87Y/Radford e.a. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{XLNET,
	title = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
	url = {http://arxiv.org/abs/1906.08237},
	doi = {10.48550/arXiv.1906.08237},
	shorttitle = {{XLNet}},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like {BERT} achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, {BERT} neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose {XLNet}, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of {BERT} thanks to its autoregressive formulation. Furthermore, {XLNet} integrates ideas from Transformer-{XL}, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, {XLNet} outperforms {BERT} on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	number = {{arXiv}:1906.08237},
	publisher = {{arXiv}},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	urldate = {2023-10-03},
	date = {2020-01-02},
        year = {2020},
	eprinttype = {arxiv},
	eprint = {1906.08237 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/RS5ATTXE/Yang e.a. - 2020 - XLNet Generalized Autoregressive Pretraining for .pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/YF9IWM4C/1906.html:text/html},
}



@inproceedings{riedl_weird_2020,
	title = {Weird {AI} Yankovic: Generating Parody Lyrics},
	url = {http://arxiv.org/abs/2009.12240},
	doi = {10.48550/arXiv.2009.12240},
	shorttitle = {Weird {AI} Yankovic},
	abstract = {Lyrics parody swaps one set of words that accompany a melody with a new set of words, preserving the number of syllables per line and the rhyme scheme. Lyrics parody generation is a challenge for controllable text generation. We show how a specialized sampling procedure, combined with backward text generation with {XLNet} can produce parody lyrics that reliably meet the syllable and rhyme scheme constraints.We introduce the Weird {AI} Yankovic system and provide a case study evaluation. We conclude with societal implications of neural lyric parody generation.},
	number = {{arXiv}:2009.12240},
	publisher = {{arXiv}},
	author = {Riedl, Mark},
	urldate = {2023-10-03},
	date = {2020-09-25},
        year = {2020},
	eprinttype = {arxiv},
	eprint = {2009.12240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/ZRSSD6KK/Riedl - 2020 - Weird AI Yankovic Generating Parody Lyrics.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/G3HUPCVC/2009.html:text/html},
}

@inproceedings{gatti_sing_2017,
	location = {Valencia, Spain},
	title = {To Sing like a Mockingbird},
	url = {http://aclweb.org/anthology/E17-2048},
	doi = {10.18653/v1/E17-2048},
	abstract = {Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a system for automatically producing a musical parody, starting from a corpus of songs. The system can automatically identify characterizing words and concepts related to a novel text, which are taken from the daily news. These concepts are then used as seeds to appropriately replace part of the original lyrics of a song, using metrical, rhyming and lexical constraints. Finally, the parody can be sung with a singing speech synthesizer, with no intervention from the user.},
	eventtitle = {Proceedings of the 15th Conference of the European Chapter of the           Association for Computational Linguistics: Volume 2, Short Papers},
	pages = {298--304},
	booktitle = {Proceedings of the 15th Conference of the European Chapter of the           Association for Computational Linguistics: Volume 2, Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Gatti, Lorenzo and Özbal, Gözde and Stock, Oliviero and Strapparava, Carlo},
	urldate = {2023-10-03},
	date = {2017},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/JUE8TICU/Gatti e.a. - 2017 - To Sing like a Mockingbird.pdf:application/pdf},
}

@paper{oliveira_weirdanalogymatic_2020,
	title = {{WeirdAnalogyMatic}: Experimenting with Analogy for Lyrics Transformation},
	url = {https://www.semanticscholar.org/paper/WeirdAnalogyMatic%3A-Experimenting-with-Analogy-for-Oliveira/3564ece8bd2b7c57ce005cc91098ebfa766b7bc2},
	shorttitle = {{WeirdAnalogyMatic}},
	abstract = {This paper is on the transformation of text relying mostly on a common analogy vector operation, computed in a distributional model, i.e., static word embeddings. Given a theme, original song lyrics have their words replaced by new ones, related to the new theme as the original words are to the original theme. As this is not enough for producing good lyrics, towards more coherent and singable text, constraints are gradually applied to the replacements. Human opinions confirmed that more balanced lyrics are obtained when there is a one-toone mapping between original words and their replacement; only content words are replaced; and the replacement has the same part-of-speech and rhythm as the original word.},
	eventtitle = {International Conference on Innovative Computing and Cloud Computing},
	author = {Oliveira, Hugo Gonçalo},
	urldate = {2023-10-03},
	date = {2020},
        year = {2020},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/KJUS76JA/Oliveira - 2020 - WeirdAnalogyMatic Experimenting with Analogy for .pdf:application/pdf},
}

@paper{rodrigues_lyrics_2022,
	title = {Lyrics Generation supported by Pre-trained Models},
	volume = {35},
	issn = {2334-0762},
	url = {https://journals.flvc.org/FLAIRS/article/view/130607},
	doi = {10.32473/flairs.v35i.130607},
        year = {2022},
	abstract = {Advancements in neural network architectures have improved the quality of several tasks in computational linguistics. Among the tasks benefited we can mention question and answer systems, dialogue systems, opinion mining and the automatic generation of texts, just to mention a few. Despite the advances, there is still room for contributions, since there are still open problems. In the case of text generation, especially in the musical genre, there are challenges for the production of texts that involve poetry and idioms. In particular, some of these challenges are linked to the treatment of metaphors and metonymy and the generation of paraphrases. This paper presents an analysis of the generation of excerpts of lyrics based on a pre-trained {GPT}-2 neural network model, after fine-tuning with two lyrics corpora, one in English and one in Portuguese. An analysis of the spelling, syntax and semantics of the generated texts are presented, as well as the discussion about the attempt to find a pattern in the sections generated by the implemented tool. Research demonstrates the potential for using such models in the generation of poetic texts.},
	journaltitle = {The International {FLAIRS} Conference Proceedings},
	shortjournal = {{FLAIRS}},
	author = {Rodrigues, Matheus Augusto and Oliveira, Alcione and Moreira, Alexandra and Possi, Maurilio},
	urldate = {2023-10-03},
	date = {2022-05-04},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/X94P7AW6/Rodrigues e.a. - 2022 - Lyrics Generation supported by Pre-trained Models.pdf:application/pdf},
}

@inproceedings{bay_text_2017,
	title = {Text Transformation Via Constraints and Word Embedding},
	url = {https://www.semanticscholar.org/paper/Text-Transformation-Via-Constraints-and-Word-Bay-Bodily/ff2975cf624280e24b874c9efbb45baf5398a143},
	abstract = {In order to provide resources for artistic communities and further the linguistic capabilities of computationally creative systems, we present a computational process for creative text transformation and evaluation. Its purpose is to help solve the fundamental problem posed by the ﬁeld of natural language generation, which is to computationally generate human-readable language. Our process entails the use of 1) vector word embedding to approximate meaning and 2) constraints to guide word replacement. We introduce intentions as objects that drive the generation of creative artefacts; a target theme, emotion, meter, or rhyme scheme may be represented via intention. Our implementation of this process, Lyrist , is oriented around poetry and song lyrics and successfully produces syntactically correct, human-voiced text. A preliminary evaluation suggests that our process successfully evokes human-recognizable sentiments and that even familiar texts are difﬁcult to recognize after undergoing transformation.},
	eventtitle = {International Conference on Innovative Computing and Cloud Computing},
	author = {Bay, Benjamin and Bodily, P. and Ventura, D.},
	urldate = {2023-10-14},
	date = {2017},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/CC2UAWDX/Bay e.a. - 2017 - Text Transformation Via Constraints and Word Embed.pdf:application/pdf},
}

@inproceedings{potash_ghostwriter_2015,
	location = {Lisbon, Portugal},
	title = {{GhostWriter}: Using an {LSTM} for Automatic Rap Lyric Generation},
	url = {https://aclanthology.org/D15-1221},
	doi = {10.18653/v1/D15-1221},
	shorttitle = {{GhostWriter}},
	eventtitle = {{EMNLP} 2015},
	pages = {1919--1924},
        year = {2015},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Potash, Peter and Romanov, Alexey and Rumshisky, Anna},
	urldate = {2023-10-14},
	date = {2015-09},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/XB8JZ9DJ/Potash e.a. - 2015 - GhostWriter Using an LSTM for Automatic Rap Lyric.pdf:application/pdf},
}

@inproceedings{liu_chipsong_2022,
	location = {Dublin, Ireland},
	title = {{ChipSong}: A Controllable Lyric Generation System for Chinese Popular Song},
	url = {https://aclanthology.org/2022.in2writing-1.13},
	doi = {10.18653/v1/2022.in2writing-1.13},
        year = {2022},
	shorttitle = {{ChipSong}},
	abstract = {In this work, we take a further step towards satisfying practical demands in Chinese lyric generation from musical short-video creators, in respect of the challenges on songs' format constraints, creating specific lyrics from open-ended inspiration inputs, and language rhyme grace. One representative detail in these demands is to control lyric format at word level, that is, for Chinese songs, creators even expect fix-length words on certain positions in a lyric to match a special melody, while previous methods lack such ability. Although recent lyric generation community has made gratifying progress, most methods are not comprehensive enough to simultaneously meet these demands. As a result, we propose {ChipSong}, which is an assisted lyric generation system built based on a Transformer-based autoregressive language model architecture, and generates controlled lyric paragraphs fit for musical short-video display purpose, by designing 1) a novel Begin-Internal-End ({BIE}) word-granularity embedding sequence with its guided attention mechanism for word-level length format control, and an explicit symbol set for sentence-level length format control; 2) an open-ended trigger word mechanism to guide specific lyric contents generation; 3) a paradigm of reverse order training and shielding decoding for rhyme control. Extensive experiments show that our {ChipSong} generates fluent lyrics, with assuring the high consistency to pre-determined control conditions.},
	eventtitle = {In2Writing 2022},
	pages = {85--95},
	booktitle = {Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Nayu and Han, Wenjing and Liu, Guangcan and Peng, Da and Zhang, Ran and Wang, Xiaorui and Ruan, Huabin},
	urldate = {2023-10-16},
	date = {2022-05},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/CINGEKB3/Liu e.a. - 2022 - ChipSong A Controllable Lyric Generation System f.pdf:application/pdf},
}

@inproceedings{xue_deeprapper_2021,
	location = {Online},
	title = {{DeepRapper}: Neural Rap Generation with Rhyme and Rhythm Modeling},
	url = {https://aclanthology.org/2021.acl-long.6},
	doi = {10.18653/v1/2021.acl-long.6},
	shorttitle = {{DeepRapper}},
        year = {2021},
	abstract = {Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance. In this paper, we develop {DeepRapper}, a Transformer-based rap generation system that can model both rhymes and rhythms. Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats. Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms. Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling. To our knowledge, {DeepRapper} is the first system to generate rap with both rhymes and rhythms. Both objective and subjective evaluations demonstrate that {DeepRapper} generates creative and high-quality raps with rhymes and rhythms.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {69--81},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Lanqing and Song, Kaitao and Wu, Duocai and Tan, Xu and Zhang, Nevin L. and Qin, Tao and Zhang, Wei-Qiang and Liu, Tie-Yan},
	urldate = {2023-10-16},
	date = {2021-08},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/4ECSF6QW/Xue e.a. - 2021 - DeepRapper Neural Rap Generation with Rhyme and R.pdf:application/pdf},
}

@book{vechtomova_generating_2018,
	title = {Generating lyrics with variational autoencoder and multi-modal artist embeddings},
	abstract = {We present a system for generating song lyrics lines conditioned on the style of a specified artist. The system uses a variational autoencoder with artist embeddings. We propose the pre-training of artist embeddings with the representations learned by a {CNN} classifier, which is trained to predict artists based on {MEL} spectrograms of their song clips. This work is the first step towards combining audio and text modalities of songs for generating lyrics conditioned on the artist's style. Our preliminary results suggest that there is a benefit in initializing artists' embeddings with the representations learned by a spectrogram classifier.},
	author = {Vechtomova, Olga and Bahuleyan, Hareesh and Ghabussi, Amirpasha and John, Vineet},
	date = {2018-12-19},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/INKARP5E/Vechtomova e.a. - 2018 - Generating lyrics with variational autoencoder and.pdf:application/pdf},
}

@inproceedings{nikolov_rapformer_2020,
	title = {Rapformer: Conditional Rap Lyrics Generation with Denoising Autoencoders},
	url = {https://www.semanticscholar.org/paper/Rapformer%3A-Conditional-Rap-Lyrics-Generation-with-Nikolov-Malmi/92c6252075053816eaffc2081d7aac4bb7ad154d},
	shorttitle = {Rapformer},
	abstract = {The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel {BERT}-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10\%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25\% of the time.},
	eventtitle = {International Conference on Natural Language Generation},
	author = {Nikolov, Nikola I. and Malmi, Eric and Northcutt, Curtis G. and Parisi, Loreto},
	urldate = {2023-10-16},
	date = {2020-12-14},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/DRBHCRQW/Nikolov e.a. - 2020 - Rapformer Conditional Rap Lyrics Generation with .pdf:application/pdf},
}

@inproceedings{gatti_automatic_2017,
	location = {Mountain View California {USA}},
	title = {Automatic Generation of Lyrics Parodies},
	isbn = {978-1-4503-4906-2},
	url = {https://dl.acm.org/doi/10.1145/3123266.3123410},
	doi = {10.1145/3123266.3123410},
	eventtitle = {{MM} '17: {ACM} Multimedia Conference},
	pages = {485--491},
	booktitle = {Proceedings of the 25th {ACM} international conference on Multimedia},
	publisher = {{ACM}},
	author = {Gatti, Lorenzo and Özbal, Gözde and Stock, Oliviero and Strapparava, Carlo},
	urldate = {2023-10-16},
	date = {2017-10-19},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/GV39CXA6/Gatti e.a. - 2017 - Automatic Generation of Lyrics Parodies.pdf:application/pdf},
}

@paper{chang_singability-enhanced_2021,
	title = {Singability-enhanced lyric generator with music style transfer},
	volume = {168},
	issn = {0140-3664},
        year = {2021},
	url = {https://www.sciencedirect.com/science/article/pii/S0140366421000104},
	doi = {10.1016/j.comcom.2021.01.002},
	abstract = {The lyrics generator should consider the context and the singability of the songs because every song expresses a story through the context of lyrics, and the lyrics should sound with the music well. Therefore, this study proposes a framework to generate the singable lyrics, and the context of lyrics should fit the given musical style. For the context, this study adopts the {GPT}-2 model which is powerful for text generation. The conditional {GPT}-2 model can be used to generate lyrics according to the given style. For suitable for singing, this study adjusts the structure and rhyme of lyrics through the use of a syntactic parser and a rhyme modification module. With automatic and human evaluations, the experimental results show that the proposed method can generate lyrics with high structural consistency, rhyme consistency, and originality according to the given music style.},
	pages = {33--53},
	journaltitle = {Computer Communications},
	shortjournal = {Computer Communications},
	author = {Chang, Jia-Wei and Hung, Jason C. and Lin, Kuan-Cheng},
	urldate = {2023-10-16},
	date = {2021-02-15},
	keywords = {{GPT}-2, Lyric generator, Music style transfer, Natural language processing},
	file = {ScienceDirect Full Text PDF:/Users/antonlintermans/Zotero/storage/TAWU2Z9X/Chang e.a. - 2021 - Singability-enhanced lyric generator with music st.pdf:application/pdf},
}

@inproceedings{ram_say_2021,
	location = {New York, {NY}, {USA}},
	title = {Say What? Collaborative Pop Lyric Generation Using Multitask Transfer Learning},
	isbn = {978-1-4503-8620-3},
	url = {https://dl.acm.org/doi/10.1145/3472307.3484175},
	doi = {10.1145/3472307.3484175},
        year =  {2021},
	series = {{HAI} '21},
	shorttitle = {Say What?},
	abstract = {Lyric generation is a popular sub-field of natural language generation that has seen growth in recent years. Pop lyrics are of unique interest due to the genre’s unique style and content, in addition to the high level of collaboration that goes on behind the scenes in the professional pop songwriting process. In this paper, we present a collaborative line-level lyric generation system that utilizes transfer-learning via the T5 transformer model, which, till date, has not been used to generate pop lyrics. By working and communicating directly with professional songwriters, we develop a model that is able to learn lyrical and stylistic tasks like rhyming, matching line beat requirements, and ending lines with specific target words. Our approach compares favorably to existing methods for multiple datasets and yields positive results from our online studies and interviews with industry songwriters.},
	pages = {165--173},
	booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
	publisher = {Association for Computing Machinery},
	author = {Ram, Naveen and Gummadi, Tanay and Bhethanabotla, Rahul and Savery, Richard J and Weinberg, Gil},
	urldate = {2023-10-16},
	date = {2021-11-09},
	keywords = {transfer learning, natural language generation, collaborative {AI}, lyric generation, pop music, transformers},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/IYSHX3BD/Ram e.a. - 2021 - Say What Collaborative Pop Lyric Generation Using.pdf:application/pdf},
}

@inproceedings{oliveira_exploring_nodate,
	title = {Exploring a Masked Language Model for Creative Text Transformation},
	abstract = {We explore a masked-language model based on {BERT} for shifting the meaning of text towards a target theme. Content words in the original text are masked and the model provides a list of ﬁlling candidates, out of which one is selected based on its similarity to the theme and constraints regarding morphology and metre. Experimentation is performed with Portuguese song lyrics and trade-offs between grammaticality, semantics, form, and novelty are analysed. We conﬁrm that {BERT} is a useful tool for creative text transformation.},
	author = {Oliveira, Hugo Goncalo},
	langid = {english},
        book = {Proceedings of the 12th International Conference on Computational Creativity (ICCC ’21)},
        ISBN = {978-989-54160-3-5},
        year = {2021},
	file = {Oliveira - Exploring a Masked Language Model for Creative Tex.pdf:/Users/antonlintermans/Zotero/storage/KMXG5LYN/Oliveira - Exploring a Masked Language Model for Creative Tex.pdf:application/pdf},
}

@article{_lyrics_nodate,
	title = {Lyrics and Vocal Melody Generation conditioned on Accompaniment},
	author = {Μελίστας, Θωμάς},
	langid = {greek},
	file = {Μελίστας - Lyrics and Vocal Melody Generation conditioned on .pdf:/Users/antonlintermans/Zotero/storage/KDA32UK5/Μελίστας - Lyrics and Vocal Melody Generation conditioned on .pdf:application/pdf},
}

@inproceedings{lau_application_2021,
	title = {Application of Machine Learning Model in Generating Song Lyrics},
	url = {https://escholarship.org/uc/item/77d7c3hh},
	abstract = {(Abstract omitted for brevity)},
	institution = {{UCLA}},
	type = {phdthesis},
	author = {Lau, Wesley},
	urldate = {2023-10-16},
	year = {2021},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/SJ8T3E38/Lau - 2021 - Application of Machine Learning Model in Generatin.pdf:application/pdf},
}

@article{oliveira_tra--lyrics_2015,
	title = {Tra-la-Lyrics 2.0: Automatic Generation of Song Lyrics on a Semantic Domain},
	volume = {6},
	url = {https://sciendo.com/article/10.1515/jagi-2015-0005},
	doi = {10.1515/jagi-2015-0005},
	shorttitle = {Tra-la-Lyrics 2.0},
	abstract = {{AbstractTra}-la-Lyrics is a system that generates song lyrics automatically. In its original version, the main focus was to produce text where stresses matched the rhythm of given melodies. There were no concerns on whether the text made sense or if the selected words shared some kind of semantic association. In this article, we describe the development of a new version of Tra-la-Lyrics, where text is generated on a semantic domain, defined by one or more seed words. This effort involved the integration of the original rhythm module of Tra-la-Lyrics in {PoeTryMe}, a generic platform that generates poetry with semantically coherent sentences. To measure our progress, the rhythm, the rhymes, and the semantic coherence in lyrics produced by the original Tra-la-Lyrics were analysed and compared with lyrics produced by the new instantiation of this system, dubbed Tra-la-Lyrics 2.0. The analysis showed that, in the lyrics by the new system, words have higher semantic association among them and with the given seeds, while the rhythm is still matched and rhymes are present. The previous analysis was complemented with a crowdsourced evaluation, where contributors answered a survey about relevant features of lyrics produced by the previous and the current versions of Tra-la-Lyrics. Though},
	pages = {87--110},
	number = {1},
	journaltitle = {Journal of Artificial General Intelligence},
	author = {Oliveira, Hugo Gonçalo},
	urldate = {2023-10-16},
	date = {2015-12-01},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/2H33RMHT/Oliveira - 2015 - Tra-la-Lyrics 2.0 Automatic Generation of Song Ly.pdf:application/pdf},
}

@article{singh_automated_2018,
	title = {{AUTOMATED} {LYRICAL} {NARRATIVE} {WRITING}},
	url = {https://scholarworks.sjsu.edu/etd_projects/617},
	doi = {https://doi.org/10.31979/etd.756z-tvbf},
	journaltitle = {Master's Projects},
	author = {Singh, Divya},
	date = {2018-04-01},
        year =  {2018},
	file = {"AUTOMATED LYRICAL NARRATIVE WRITING" by Divya Singh:/Users/antonlintermans/Zotero/storage/TV6B2875/617.html:text/html;Volledige Tekst:/Users/antonlintermans/Zotero/storage/AVK2GJ58/Singh - 2018 - AUTOMATED LYRICAL NARRATIVE WRITING.pdf:application/pdf},
}

@misc{andersson_ai_nodate,
	title = {{AI} generated parody lyrics},
	abstract = {This essay concerns {AI} generation of parody lyrics, based on an existing program called Weird {AI} Yankovic (hereafter {WAIY}) that is developed by Mark O. Riedl at the Georgia Institute of Technology. It is a Python program that loads the powerful language models {GPT}-2 and {XLNet} and takes a short parody context description as well as an existing song text as input. While the syllable and rhyme structure are retained, the words are replaced with newly generated text based on the entered context. The result is new lyrics that can be sung to the original melody but are intended to gain comical qualities given the new content.},
	author = {Andersson, Daniel},
	langid = {english},
        year = {fall 2021},
	file = {Andersson - AI generated parody lyrics.pdf:/Users/antonlintermans/Zotero/storage/9XGBAZ5P/Andersson - AI generated parody lyrics.pdf:application/pdf},
}

@inproceedings{sasaki_pilot_2021,
	location = {Cham},
	title = {Pilot Study: Does Phonological Similarity of Words Enhance Humor in “Soramimi” Parody Songs?},
	isbn = {978-3-030-78635-9},
	doi = {10.1007/978-3-030-78635-9_77},
	series = {Communications in Computer and Information Science},
	shorttitle = {Pilot Study},
	abstract = {Humor is essential to establish more natural and enjoyable human–computer interactions, and researchers have been working on developing a way to automatically generate humor. This study aims to explore a better way to automatically generate Japanese common humor “soramimi”, and understand how the humor occurs. Soramimi is a type of parody song in which the original lyrics are replaced by different words that have similar pronunciations. Although a previous study proposed an algorithm to replace input text with homophonic soramimi text, the mechanism of soramimi humor is still unclear. Based on the incongruity-resolution model, we hypothesized that phonological similarity between the parody and the original lyrics enhances humor in soramimi. A subjective experiment was conducted in which the phonological similarity and humor of fifteen soramimi parody lyrics were evaluated. The results indicated that the phonological similarity of soramimi was positively correlated with its humorousness. Exploring other factors that affect humorousness and the development of an automatic generation system for soramimi lyrics based on the identified factors are topics for our future research.},
	pages = {603--608},
	booktitle = {{HCI} International 2021 - Posters},
	publisher = {Springer International Publishing},
	author = {Sasaki, Masaru and Shimaya, Jiro and Nakamura, Yutaka},
	editor = {Stephanidis, Constantine and Antona, Margherita and Ntoa, Stavroula},
	date = {2021},
	langid = {english},
	keywords = {Humor, Incongruity-resolution model, Soramimi},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/EFQU2MST/Sasaki e.a. - 2021 - Pilot Study Does Phonological Similarity of Words.pdf:application/pdf},
}

@inproceedings{manjavacas_generation_2019,
	location = {Tokyo, Japan},
	title = {Generation of Hip-Hop Lyrics with Hierarchical Modeling and Conditional Templates},
	url = {https://aclanthology.org/W19-8638},
	doi = {10.18653/v1/W19-8638},
	abstract = {This paper addresses Hip-Hop lyric generation with conditional Neural Language Models. We develop a simple yet effective mechanism to extract and apply conditional templates from text snippets, and show—on the basis of a large-scale crowd-sourced manual evaluation—that these templates significantly improve the quality and realism of the generated snippets. Importantly, the proposed approach enables end-to-end training, targeting formal properties of text such as rhythm and rhyme, which are central characteristics of rap texts. Additionally, we explore how generating text at different scales (e.g. character-level or word-level) affects the quality of the output. We find that a hybrid form—a hierarchical model that aims to integrate Language Modeling at both word and character-level scales—yields significant improvements in text quality, yet surprisingly, cannot exploit conditional templates to their fullest extent. Our findings highlight that text generation models based on Recurrent Neural Networks ({RNN}) are sensitive to the modeling scale and call for further research on the observed differences in effectiveness of the conditioning mechanism at different scales.},
	eventtitle = {{INLG} 2019},
	pages = {301--310},
	booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
	publisher = {Association for Computational Linguistics},
	author = {Manjavacas, Enrique and Kestemont, Mike and Karsdorp, Folgert},
	editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
	urldate = {2023-11-12},
	date = {2019-10},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/CEEWVVJ5/Manjavacas et al. - 2019 - Generation of Hip-Hop Lyrics with Hierarchical Mod.pdf:application/pdf},
}



@misc{zhang_tractable_2023,
	title = {Tractable Control for Autoregressive Language Generation},
	url = {http://arxiv.org/abs/2304.07438},
	doi = {10.48550/arXiv.2304.07438},
	abstract = {Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution \$\{{\textbackslash}Pr\}({\textbackslash}text\{text\} {\textbar} {\textbackslash}alpha)\$ is intractable for even the simplest lexical constraints \${\textbackslash}alpha\$. To overcome this challenge, we propose to use tractable probabilistic models ({TPMs}) to impose lexical constraints in autoregressive text generation models, which we refer to as {GeLaTo} (Generating Language with Tractable Constraints). To demonstrate the effectiveness of this framework, we use distilled hidden Markov models, where we can efficiently compute \$\{{\textbackslash}Pr\}({\textbackslash}text\{text\} {\textbar} {\textbackslash}alpha)\$, to guide autoregressive generation from {GPT}2. {GeLaTo} achieves state-of-the-art performance on challenging benchmarks for constrained text generation (e.g., {CommonGen}), beating various strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive {TPMs}.},
	number = {{arXiv}:2304.07438},
	publisher = {{arXiv}},
	author = {Zhang, Honghua and Dang, Meihua and Peng, Nanyun and Broeck, Guy Van den},
	urldate = {2023-09-30},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2304.07438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/GKYEL2PV/Zhang e.a. - 2023 - Tractable Control for Autoregressive Language Gene.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/R96KW65V/2304.html:text/html},
}

@misc{dang_tractable_2023,
	title = {Tractable and Expressive Generative Models of Genetic Variation Data},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.05.16.541036v1},
	doi = {10.1101/2023.05.16.541036},
	abstract = {Population genetic studies often rely on artificial genomes ({AGs}) simulated by generative models of genetic data. In recent years, unsupervised learning models, based on hidden Markov models, deep generative adversarial networks, restricted Boltzmann machines, and variational autoencoders, have gained popularity due to their ability to generate {AGs} closely resembling empirical data. These models, however, present a tradeoff between expressivity and tractability. Here, we propose to use hidden Chow-Liu trees ({HCLTs}) and their representation as probabilistic circuits ({PCs}) as a solution to this tradeoff. We first learn an {HCLT} structure that captures the long-range dependencies among {SNPs} in the training data set. We then convert the {HCLT} to its equivalent {PC} as a means of supporting tractable and efficient probabilistic inference. The parameters in these {PCs} are inferred with an expectation-maximization algorithm using the training data. Compared to other models for generating {AGs}, {HCLT} obtains the largest log-likelihood on test genomes across {SNPs} chosen across the genome and from a contiguous genomic region. Moreover, the {AGs} generated by {HCLT} more accurately resemble the source data set in their patterns of allele frequencies, linkage disequilibrium, pairwise haplotype distances, and population structure. This work not only presents a new and robust {AG} simulator but also manifests the potential of {PCs} in population genetics.},
	publisher = {{bioRxiv}},
	author = {Dang, Meihua and Liu, Anji and Wei, Xinzhu and Sankararaman, Sriram and Broeck, Guy Van den},
	urldate = {2023-10-03},
	date = {2023-05-18},
	langid = {english},
	note = {Pages: 2023.05.16.541036
Section: New Results},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/PTHN36G6/Dang e.a. - 2023 - Tractable and Expressive Generative Models of Gene.pdf:application/pdf},
}

@misc{lin_commongen_2020,
	title = {{CommonGen}: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},
	url = {http://arxiv.org/abs/1911.03705},
	doi = {10.48550/arXiv.1911.03705},
	shorttitle = {{CommonGen}},
	abstract = {Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, {CommonGen} associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., \{dog, frisbee, catch, throw\}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., "a man throws a frisbee and his dog catches it"). The {CommonGen} task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance. Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as {CommonsenseQA} by generating additional context.},
	number = {{arXiv}:1911.03705},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
	urldate = {2023-10-03},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {1911.03705 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/AP3JE86J/Lin e.a. - 2020 - CommonGen A Constrained Text Generation Challenge.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/F4SFE5YE/1911.html:text/html},
}

@misc{lu_insnet_2022,
	title = {{InsNet}: An Efficient, Flexible, and Performant Insertion-based Text Generation Model},
	url = {http://arxiv.org/abs/2102.11008},
	doi = {10.48550/arXiv.2102.11008},
	shorttitle = {{InsNet}},
	abstract = {We propose {InsNet}, an expressive insertion-based text generator with efficient training and flexible decoding (parallel or sequential). Unlike most existing insertion-based text generation works that require re-encoding of the context after each insertion operation and thus are inefficient to train, {InsNet} only requires one pass of context encoding for the entire sequence during training by introducing a novel insertion-oriented position encoding and a light-weighted slot representation strategy to enable computation sharing. Furthermore, we propose an algorithm {InsNet}-Dinic to better determine the parallelization of insertion operations that provides a controllable switch between parallel and sequential decoding, making it flexible to handle more parallelizable tasks such as machine translation with efficient decoding, or less parallelizable tasks such as open-domain text generation to guarantee high-quality outputs. Experiments on two lexically constrained text generation datasets and three machine translation datasets demonstrate {InsNet}'s advantages over previous insertion-based methods in terms of training speed, inference efficiency, and generation quality.},
	number = {{arXiv}:2102.11008},
	publisher = {{arXiv}},
	author = {Lu, Sidi and Meng, Tao and Peng, Nanyun},
	urldate = {2023-10-03},
	date = {2022-10-15},
	eprinttype = {arxiv},
	eprint = {2102.11008 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/YNLIPDDN/Lu e.a. - 2022 - InsNet An Efficient, Flexible, and Performant Ins.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/28EMGH6W/2102.html:text/html},
}

@misc{lu_neurologic_2021,
	title = {{NeuroLogic} Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints},
	url = {http://arxiv.org/abs/2010.12884},
	doi = {10.48550/arXiv.2010.12884},
	shorttitle = {{NeuroLogic} Decoding},
	abstract = {Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose {NeuroLogic} Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that {NeuroLogic} Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with {NeuroLogic} Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.},
	number = {{arXiv}:2010.12884},
	publisher = {{arXiv}},
	author = {Lu, Ximing and West, Peter and Zellers, Rowan and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	urldate = {2023-10-03},
	date = {2021-04-20},
	eprinttype = {arxiv},
	eprint = {2010.12884 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/2DYFQ4PV/Lu e.a. - 2021 - NeuroLogic Decoding (Un)supervised Neural Text Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/8EIDFBIG/2010.html:text/html},
}

@misc{meng_controllable_2022,
	title = {Controllable Text Generation with Neurally-Decomposed Oracle},
	url = {http://arxiv.org/abs/2205.14219},
	doi = {10.48550/arXiv.2205.14219},
	abstract = {We propose a general and efficient framework to control auto-regressive generation models with {NeurAlly}-Decomposed Oracle ({NADO}). Given a pre-trained base language model and a sequence-level boolean oracle function, we propose to decompose the oracle function into token-level guidance to steer the base model in text generation. Specifically, the token-level guidance is approximated by a neural model trained with examples sampled from the base model, demanding no additional auxiliary labeled data. Based on posterior regularization, we present the closed-form optimal solution to incorporate the token-level guidance into the base model for controllable generation. We further provide a theoretical analysis of how the approximation quality of {NADO} affects the controllable generation results. Experiments conducted on two applications: (1) text generation with lexical constraints and (2) machine translation with formality control demonstrate that our framework efficiently guides the base model towards the given oracle while maintaining high generation quality.},
	number = {{arXiv}:2205.14219},
	publisher = {{arXiv}},
	author = {Meng, Tao and Lu, Sidi and Peng, Nanyun and Chang, Kai-Wei},
	urldate = {2023-10-03},
	date = {2022-10-20},
	eprinttype = {arxiv},
	eprint = {2205.14219 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/439JEPQL/Meng e.a. - 2022 - Controllable Text Generation with Neurally-Decompo.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/92YPQRD2/2205.html:text/html},
}

@inproceedings{post_fast_2018,
	location = {New Orleans, Louisiana},
	title = {Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation},
	url = {https://aclanthology.org/N18-1119},
	doi = {10.18653/v1/N18-1119},
	abstract = {The end-to-end nature of neural machine translation ({NMT}) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm's remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and {BLEU} scores. Our implementation is available as part of Sockeye.},
	eventtitle = {{NAACL}-{HLT} 2018},
	pages = {1314--1324},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Post, Matt and Vilar, David},
	urldate = {2023-10-03},
	date = {2018-06},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/NHJUVC2S/Post en Vilar - 2018 - Fast Lexically Constrained Decoding with Dynamic B.pdf:application/pdf},
}

@inproceedings{yang_fudge_2021,
	title = {{FUDGE}: Controlled Text Generation With Future Discriminators},
	url = {http://arxiv.org/abs/2104.05218},
	doi = {10.18653/v1/2021.naacl-main.276},
	shorttitle = {{FUDGE}},
	abstract = {We propose Future Discriminators for Generation ({FUDGE}), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, {FUDGE} enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. {FUDGE} learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that {FUDGE} models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, {FUDGE} can easily compose predictors for multiple desired attributes. We evaluate {FUDGE} on three tasks -- couplet completion in poetry, topic control in language generation, and formality change in machine translation -- and observe gains in all three tasks.},
	pages = {3511--3535},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	author = {Yang, Kevin and Klein, Dan},
	urldate = {2023-10-03},
	date = {2021},
	eprinttype = {arxiv},
	eprint = {2104.05218 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/PN6YRKT5/Yang en Klein - 2021 - FUDGE Controlled Text Generation With Future Disc.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/RRN9GAST/2104.html:text/html},
}

@misc{zhang_pointer_2020,
	title = {{POINTER}: Constrained Progressive Text Generation via Insertion-based Generative Pre-training},
	url = {http://arxiv.org/abs/2005.00558},
	doi = {10.48550/arXiv.2005.00558},
	shorttitle = {{POINTER}},
	abstract = {Large-scale pre-trained language models, such as {BERT} and {GPT}-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present {POINTER} ({PrOgressive} {INsertion}-based {TransformER}), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields an empirically logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that {POINTER} achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research (https://github.com/dreasysnail/{POINTER}).},
	number = {{arXiv}:2005.00558},
	publisher = {{arXiv}},
	author = {Zhang, Yizhe and Wang, Guoyin and Li, Chunyuan and Gan, Zhe and Brockett, Chris and Dolan, Bill},
	urldate = {2023-10-03},
	date = {2020-09-26},
	eprinttype = {arxiv},
	eprint = {2005.00558 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/T4IBDAUI/Zhang e.a. - 2020 - POINTER Constrained Progressive Text Generation v.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/6SUUTFFX/2005.html:text/html},
}

@article{ma_switch-gpt_2022,
	title = {Switch-{GPT}: An Effective Method for Constrained Text Generation under Few-Shot Settings (Student Abstract)},
	volume = {36},
	rights = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21642},
	doi = {10.1609/aaai.v36i11.21642},
	shorttitle = {Switch-{GPT}},
	abstract = {In real-world applications of natural language generation, target sentences are often required to satisfy some lexical constraints. However, the success of most neural-based models relies heavily on data, which is infeasible for data-scarce new domains. In this work, we present {FewShotAmazon}, the first benchmark for the task of Constrained Text Generation under few-shot settings on multiple domains. Further, we propose the Switch-{GPT} model, in which we utilize the strong language modeling capacity of {GPT}-2 to generate fluent and well-formulated sentences, while using a light attention module to decide which constraint to attend to at each step. Experiments show that the proposed Switch-{GPT} model is effective and remarkably outperforms the baselines. Codes will be available at https://github.com/chang-github-00/Switch-{GPT}.},
	pages = {13011--13012},
	number = {11},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Ma, Chang and Zhang, Song and Shen, Gehui and Deng, Zhihong},
	urldate = {2023-10-03},
	date = {2022-06-28},
	langid = {english},
	note = {Number: 11},
	keywords = {Text Generation},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/Y2J2MGHW/Ma e.a. - 2022 - Switch-GPT An Effective Method for Constrained Te.pdf:application/pdf},
}

@misc{hu_toward_2018,
	title = {Toward Controlled Generation of Text},
	url = {http://arxiv.org/abs/1703.00955},
	doi = {10.48550/arXiv.1703.00955},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	number = {{arXiv}:1703.00955},
	publisher = {{arXiv}},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	urldate = {2023-10-03},
	date = {2018-09-12},
	eprinttype = {arxiv},
	eprint = {1703.00955 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/QU5LQR3T/Hu e.a. - 2018 - Toward Controlled Generation of Text.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/CYZKM9T9/1703.html:text/html},
}

@inproceedings{ko_assessing_2020,
	location = {Dublin, Ireland},
	title = {Assessing Discourse Relations in Language Generation from {GPT}-2},
	url = {https://aclanthology.org/2020.inlg-1.8},
	abstract = {Recent advances in {NLP} have been attributed to the emergence of large-scale pre-trained language models. {GPT}-2, in particular, is suited for generation tasks given its left-to-right language modeling objective, yet the linguistic quality of its generated text has largely remain unexplored. Our work takes a step in understanding {GPT}-2's outputs in terms of discourse coherence. We perform a comprehensive study on the validity of explicit discourse relations in {GPT}-2's outputs under both organic generation and fine-tuned scenarios. Results show {GPT}-2 does not always generate text containing valid discourse relations; nevertheless, its text is more aligned with human expectation in the fine-tuned scenario. We propose a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.},
	eventtitle = {{INLG} 2020},
	pages = {52--59},
	booktitle = {Proceedings of the 13th International Conference on Natural Language Generation},
	publisher = {Association for Computational Linguistics},
	author = {Ko, Wei-Jen and Li, Junyi Jessy},
	urldate = {2023-10-03},
	date = {2020-12},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/RTAF3GYA/Ko en Li - 2020 - Assessing Discourse Relations in Language Generati.pdf:application/pdf},
}

@inproceedings{anderson_guided_2017,
	location = {Copenhagen, Denmark},
	title = {Guided Open Vocabulary Image Captioning with Constrained Beam Search},
	url = {https://aclanthology.org/D17-1098},
	doi = {10.18653/v1/D17-1098},
	abstract = {Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on {MSCOCO} (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated {ImageNet} captions by leveraging ground-truth labels.},
	eventtitle = {{EMNLP} 2017},
	pages = {936--945},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
	urldate = {2024-01-06},
	date = {2017-09},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/2T9JLK33/Anderson e.a. - 2017 - Guided Open Vocabulary Image Captioning with Const.pdf:application/pdf},
}

@inproceedings{hokamp_lexically_2017,
	location = {Vancouver, Canada},
	title = {Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},
	url = {http://aclweb.org/anthology/P17-1141},
	doi = {10.18653/v1/P17-1141},
	abstract = {We present Grid Beam Search ({GBS}), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model’s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that {GBS} can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, {GBS} can be used to achieve significant gains in performance in domain adaptation scenarios.},
	eventtitle = {Proceedings of the 55th Annual Meeting of the Association for           Computational Linguistics (Volume 1: Long Papers)},
	pages = {1535--1546},
	booktitle = {Proceedings of the 55th Annual Meeting of the Association for           Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Hokamp, Chris and Liu, Qun},
	urldate = {2024-01-06},
	date = {2017},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/PG9WCULB/Hokamp en Liu - 2017 - Lexically Constrained Decoding for Sequence Genera.pdf:application/pdf},
}

@inproceedings{lu_neurologic_2022,
	location = {Seattle, United States},
	title = {{NeuroLogic} A*esque Decoding: Constrained Text Generation with Lookahead Heuristics},
	url = {https://aclanthology.org/2022.naacl-main.57},
	doi = {10.18653/v1/2022.naacl-main.57},
	shorttitle = {{NeuroLogic} A*esque Decoding},
	abstract = {The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A{\textasciicircum}* search algorithm, we propose {NeuroLogic} A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on {NeuroLogic} decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. {NeuroLogic} A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.},
	eventtitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	pages = {780--799},
	booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Ximing and Welleck, Sean and West, Peter and Jiang, Liwei and Kasai, Jungo and Khashabi, Daniel and Le Bras, Ronan and Qin, Lianhui and Yu, Youngjae and Zellers, Rowan and Smith, Noah and Choi, Yejin},
	urldate = {2024-01-06},
	date = {2022},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/8IU9GQM6/Lu e.a. - 2022 - NeuroLogic Aesque Decoding Constrained Text Gene.pdf:application/pdf},
}

@article{pascual_directed_2020,
	title = {Directed Beam Search: Plug-and-Play Lexically Constrained Language Generation},
	url = {https://www.semanticscholar.org/paper/Directed-Beam-Search%3A-Plug-and-Play-Lexically-Pascual-Egressy/568deb4817e7633483c93be3f50dcf51493963ff},
	shorttitle = {Directed Beam Search},
	abstract = {Large pre-trained language models are capable of generating realistic text. However, controlling these models so that the generated text satisfies lexical constraints, i.e., contains specific words, is a challenging problem. Given that state-of-the-art language models are too large to be trained from scratch in a manageable time, it is desirable to control these models without re-training them. Methods capable of doing this are called plug-and-play. Recent plug-and-play methods have been successful in constraining small bidirectional language models as well as forward models in tasks with a restricted search space, e.g., machine translation. However, controlling large transformer-based models to meet lexical constraints without re-training them remains a challenge. In this work, we propose Directed Beam Search ({DBS}), a plug-and-play method for lexically constrained language generation. Our method can be applied to any language model, is easy to implement and can be used for general language generation. In our experiments we use {DBS} to control {GPT}-2. We demonstrate its performance on keyword-to-phrase generation and we obtain comparable results as a state-of-the-art non-plug-and-play model for lexically constrained story generation.},
	journaltitle = {{ArXiv}},
	author = {Pascual, Damian and Egressy, Béni and Bolli, Florian and Wattenhofer, Roger},
	urldate = {2024-01-06},
	date = {2020-12-31},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/5K6R88RU/Pascual e.a. - 2020 - Directed Beam Search Plug-and-Play Lexically Cons.pdf:application/pdf},
}

@inproceedings{hu_improved_2019,
	location = {Minneapolis, Minnesota},
	title = {Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting},
	url = {http://aclweb.org/anthology/N19-1090},
	doi = {10.18653/v1/N19-1090},
	abstract = {Lexically-constrained sequence decoding allows for explicit positive or negative phrase-based constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting. We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three.},
	eventtitle = {Proceedings of the 2019 Conference of the North},
	pages = {839--850},
	booktitle = {Proceedings of the 2019 Conference of the North},
	publisher = {Association for Computational Linguistics},
	author = {Hu, J. Edward and Khayrallah, Huda and Culkin, Ryan and Xia, Patrick and Chen, Tongfei and Post, Matt and Van Durme, Benjamin},
	urldate = {2024-01-06},
	date = {2019},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/42JLEB94/Hu e.a. - 2019 - Improved Lexically Constrained Decoding for Transl.pdf:application/pdf},
}

@inproceedings{li_rigid_2020,
	location = {Online},
	title = {Rigid Formats Controlled Text Generation},
	url = {https://aclanthology.org/2020.acl-main.68},
	doi = {10.18653/v1/2020.acl-main.68},
	abstract = {Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, {SongCi} (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named {SongNet} to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.},
	eventtitle = {{ACL} 2020},
	pages = {742--751},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Li, Piji and Zhang, Haisong and Liu, Xiaojiang and Shi, Shuming},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-01-06},
	date = {2020-07},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/EIB2BMQW/Li e.a. - 2020 - Rigid Formats Controlled Text Generation.pdf:application/pdf},
}

@article{liao_gpt-based_2019,
	title = {{GPT}-based Generation for Classical Chinese Poetry},
	url = {https://www.semanticscholar.org/paper/GPT-based-Generation-for-Classical-Chinese-Poetry-Liao-Wang/83b56c3c7a61767bd88d85796aa5dbc4976912c3},
	abstract = {We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model ({GPT}). The method adopts a simple {GPT} model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, Lushi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ {GPT} in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry.},
	journaltitle = {{ArXiv}},
	author = {Liao, Yi and Wang, Yasheng and Liu, Qun and Jiang, Xin},
	urldate = {2024-01-06},
	date = {2019-06-29},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/43A9N6V8/Liao e.a. - 2019 - GPT-based Generation for Classical Chinese Poetry.pdf:application/pdf},
}


@misc{dathathri_plug_2020,
	title = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},
	url = {http://arxiv.org/abs/1912.02164},
	doi = {10.48550/arXiv.1912.02164},
	shorttitle = {Plug and Play Language Models},
	abstract = {Large transformer-based language models ({LMs}) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model ({PPLM}) for controllable language generation, which combines a pretrained {LM} with one or more simple attribute classifiers that guide text generation without any further training of the {LM}. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the {LM}. Sampling entails a forward and backward pass in which gradients from the attribute model push the {LM}'s hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. {PPLMs} are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
	number = {{arXiv}:1912.02164},
	publisher = {{arXiv}},
	author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
	urldate = {2024-01-08},
	date = {2020-03-03},
	eprinttype = {arxiv},
	eprint = {1912.02164 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/LF8PMQPZ/Dathathri e.a. - 2020 - Plug and Play Language Models A Simple Approach t.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/5Q2H6KMJ/1912.html:text/html},
}


@inproceedings{chen_relation-constrained_2022,
	title = {Relation-Constrained Decoding for Text Generation},
	url = {https://www.semanticscholar.org/paper/Relation-Constrained-Decoding-for-Text-Generation-Chen-Yang/f55b57e078f852ac200245689835783ecf07f2de},
	abstract = {The dominant paradigm for neural text generation nowadays is seq2seq learning with large-scale pretrained language models. However, it is usually difficult to manually constrain the generation process of these models. Prior studies have introduced Lexically Constrained Decoding ({LCD}) to ensure the presence of pre-specified words or phrases in the output. However, simply applying lexical constraints has no guarantee of the grammatical or semantic relations between words. Thus, more elaborate constraints are needed. To this end, we first propose a new constrained decoding scenario named Relation-Constrained Decoding ({RCD}) , which requires the model’s output to contain several given word pairs with respect to the given relations between them. For this scenario, we present a novel plug-and-play decoding algorithm named {RE} lation-guided probability S urgery and b E am {AL} location ({RESEAL}), which can handle different categories of relations, e.g., syn-tactical relations or factual relations. Moreover, {RESEAL} can adaptively “reseal” the relations to form a high-quality sentence, which can be applied to the inference stage of any autoregressive text generation model. To evaluate our method, we first construct an {RCD} benchmark based on dependency relations from treebanks with annotated dependencies. Experimental results demonstrate that our approach can achieve better preservation of the input dependency relations compared to previous methods. To further illustrate the effectiveness of {RESEAL}, we apply our method to three downstream tasks: sentence summarization, fact-based text editing, and data-to-text generation. We observe an improvement in generation quality. The source code is available at https://github.com/{CasparSwift}/{RESEAL} .},
	eventtitle = {Neural Information Processing Systems},
	author = {Chen, X. and Yang, Zhixian and Wan, Xiaojun},
	urldate = {2024-01-15},
	date = {2022},
}


@inproceedings{bastan_neurostructural_2023,
	location = {Toronto, Canada},
	title = {{NEUROSTRUCTURAL} {DECODING}: Neural Text Generation with Structural Constraints},
	url = {https://aclanthology.org/2023.acl-long.528},
	doi = {10.18653/v1/2023.acl-long.528},
	shorttitle = {{NEUROSTRUCTURAL} {DECODING}},
	abstract = {Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints.We address this gap by introducing {NeuroStructural} Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build {NeuroStructural} Decoding on the {NeuroLogic} Decoding (Lu etal. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object)during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences. Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding.},
	eventtitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {9496--9510},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Bastan, Mohaddeseh and Surdeanu, Mihai and Balasubramanian, Niranjan},
	urldate = {2024-01-15},
	date = {2023},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/2ZY376D2/Bastan e.a. - 2023 - NEUROSTRUCTURAL DECODING Neural Text Generation w.pdf:application/pdf},
}
@misc{kumar_controlled_2021,
	title = {Controlled Text Generation as Continuous Optimization with Multiple Constraints},
	url = {http://arxiv.org/abs/2108.01850},
	doi = {10.48550/arXiv.2108.01850},
	abstract = {As large-scale language model pretraining pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via fine-tuning remains the popular approach, it incurs a significant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose {MuCoCO} -- a flexible and modular algorithm for controllable inference from pretrained models. We formulate the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe significant improvements over baselines.},
	number = {{arXiv}:2108.01850},
	publisher = {{arXiv}},
	author = {Kumar, Sachin and Malmi, Eric and Severyn, Aliaksei and Tsvetkov, Yulia},
	urldate = {2024-01-22},
	date = {2021-08-04},
	eprinttype = {arxiv},
	eprint = {2108.01850 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/FJCXJC56/Kumar e.a. - 2021 - Controlled Text Generation as Continuous Optimizat.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/5ET9I2WE/2108.html:text/html},
}


@article{zheng_toward_2023,
	title = {Toward Unified Controllable Text Generation via Regular Expression Instruction},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.10447},
	doi = {10.48550/ARXIV.2309.10447},
	abstract = {Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction ({REI}), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our {REI} supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines.},
	author = {Zheng, Xin and Lin, Hongyu and Han, Xianpei and Sun, Le},
	urldate = {2024-01-15},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/9CWD58YS/Zheng e.a. - 2023 - Toward Unified Controllable Text Generation via Re.pdf:application/pdf},
}


@inproceedings{qin_cold_2022,
	title = {{COLD} Decoding: Energy-based Constrained Text Generation with Langevin Dynamics},
	url = {https://openreview.net/forum?id=TiZYrQ-mPup},
	shorttitle = {{COLD} Decoding},
	abstract = {Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics ({COLD}), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. {COLD} decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
	urldate = {2024-01-19},
	date = {2022-05-16},
	langid = {english},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/L4PJ7NHL/Qin e.a. - 2022 - COLD Decoding Energy-based Constrained Text Gener.pdf:application/pdf},
}


@article{zhou_controlled_2023,
	title = {Controlled Text Generation with Natural Language Instructions},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2304.14293},
	doi = {10.48550/ARXIV.2304.14293},
	abstract = {Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present {InstructCTG}, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf {NLP} tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, {InstructCTG} is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, {InstructCTG} allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.},
	author = {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Wilcox, Ethan and Cotterell, Ryan and Sachan, Mrinmaya},
	urldate = {2024-01-15},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/S65DH84B/Zhou e.a. - 2023 - Controlled Text Generation with Natural Language I.pdf:application/pdf},
}

@article{Mattas2023ChatGPT:,title={ChatGPT: A Study of AI Language Processing and its Implications},author={Puranjay Savar Mattas},journal={International Journal of Research Publication and Reviews},year={2023},doi={10.55248/gengpi.2023.4218}}

@article{Macdonald2023Can,
title={Can ChatGPT draft a research article? An example of population-level vaccine effectiveness analysis},
author={Calum Macdonald and D. Adeloye and Aziz Sheikh and I. Rudan},
journal={Journal of Global Health},
year={2023},
volume={13},
doi={10.7189/jogh.13.01003}
}

@article{AlAfnan2023ChatGPT,title={ChatGPT as an Educational Tool: Opportunities, Challenges, and Recommendations for Communication, Business Writing, and Composition Courses},author={Mohammad Awad AlAfnan and Samira Dishari and Marina Jovic and Koba Lomidze},journal={Journal of Artificial Intelligence and Technology},year={2023},doi={10.37965/jait.2023.0184}}

@online{googlebard2023,
  author = {{Google}},
  title = {Google Bard},
  year = {2023},
  url = {https://bard.google.com},
  note = {Accessed: January 25, 2024},
  urldate = {2024-01-25}
}

@misc{gemini_team_gemini_2023,
	title = {Gemini: A Family of Highly Capable Multimodal Models},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	shorttitle = {Gemini},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark {MMLU}, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
	number = {{arXiv}:2312.11805},
	publisher = {{arXiv}},
	author = {Gemini Team and others},
	urldate = {2024-01-25},
	date = {2023-12-18},
	eprinttype = {arxiv},
	eprint = {2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/DKU5GYPF/Gemini Team e.a. - 2023 - Gemini A Family of Highly Capable Multimodal Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/PGYDTHHI/2312.html:text/html},
}


@misc{touvron_llama_2023,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and others},
	urldate = {2023-10-16},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/SXTT65WH/Touvron e.a. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/8TGGYCIG/2307.html:text/html},
}

@misc{lo2022gpoet2,
      title={GPoeT-2: A GPT-2 Based Poem Generator}, 
      author={Kai-Ling Lo and Rami Ariss and Philipp Kurz},
      year={2022},
      eprint={2205.08847},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{van-de-cruys-2020-automatic,
    title = "Automatic Poetry Generation from Prosaic Text",
    author = "Van de Cruys, Tim",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.223",
    doi = "10.18653/v1/2020.acl-main.223",
    pages = "2471--2480",
    abstract = "In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.",
}

@inproceedings{hopkins-kiela-2017-automatically,
    title = "Automatically Generating Rhythmic Verse with Neural Networks",
    author = "Hopkins, Jack  and
      Kiela, Douwe",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1016",
    doi = "10.18653/v1/P17-1016",
    pages = "168--178",
    abstract = "We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54{\%} of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.",
}

@inproceedings{popescu-belis-etal-2022-constrained,
    title = "Constrained Language Models for Interactive Poem Generation",
    author = "Popescu-Belis, Andrei  and
      Atrio, {\`A}lex  and
      Minder, Valentin  and
      Xanthos, Aris  and
      Luthier, Gabriel  and
      Mattei, Simon  and
      Rodriguez, Antonio",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.377",
    pages = "3519--3529",
    abstract = "This paper describes a system for interactive poem generation, which combines neural language models (LMs) for poem generation with explicit constraints that can be set by users on form, topic, emotion, and rhyming scheme. LMs cannot learn such constraints from the data, which is scarce with respect to their needs even for a well-resourced language such as French. We propose a method to generate verses and stanzas by combining LMs with rule-based algorithms, and compare several approaches for adjusting the words of a poem to a desired combination of topics or emotions. An approach to automatic rhyme setting using a phonetic dictionary is proposed as well. Our system has been demonstrated at public events, and log analysis shows that users found it engaging.",
}

@inproceedings{uthus-etal-2022-augmenting,
    title = "Augmenting Poetry Composition with {V}erse by {V}erse",
    author = "Uthus, David  and
      Voitovich, Maria  and
      Mical, R.J.",
    editor = "Loukina, Anastassia  and
      Gangadharaiah, Rashmi  and
      Min, Bonan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track",
    month = jul,
    year = "2022",
    address = "Hybrid: Seattle, Washington + Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-industry.3",
    doi = "10.18653/v1/2022.naacl-industry.3",
    pages = "18--26",
    abstract = "We describe Verse by Verse, our experiment in augmenting the creative process of writing poetry with an AI. We have created a group of AI poets, styled after various American classic poets, that are able to offer as suggestions generated lines of verse while a user is composing a poem. In this paper, we describe the underlying system to offer these suggestions. This includes a generative model, which is tasked with generating a large corpus of lines of verse offline and which are then stored in an index, and a dual-encoder model that is tasked with recommending the next possible set of verses from our index given the previous line of verse.",
}

@inproceedings{popescu-belis-etal-2023-gpoet,
    title = "{GP}oe{T}: a Language Model Trained for Rhyme Generation on Synthetic Data",
    author = "Popescu-Belis, Andrei  and
      Atrio, {\`A}lex R.  and
      Bernath, Bastien  and
      Boisson, Etienne  and
      Ferrari, Teo  and
      Theimer-Lienhard, Xavier  and
      Vernikos, Giorgos",
    editor = "Degaetano-Ortlieb, Stefania  and
      Kazantseva, Anna  and
      Reiter, Nils  and
      Szpakowicz, Stan",
    booktitle = "Proceedings of the 7th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.latechclfl-1.2",
    doi = "10.18653/v1/2023.latechclfl-1.2",
    pages = "10--20",
    abstract = "Poem generation with language models requires the modeling of rhyming patterns. We propose a novel solution for learning to rhyme, based on synthetic data generated with a rule-based rhyming algorithm. The algorithm and an evaluation metric use a phonetic dictionary and the definitions of perfect and assonant rhymes. We fine-tune a GPT-2 English model with 124M parameters on 142 MB of natural poems and find that this model generates consecutive rhymes infrequently (11{\%}). We then fine-tune the model on 6 MB of synthetic quatrains with consecutive rhymes (AABB) and obtain nearly 60{\%} of rhyming lines in samples generated by the model. Alternating rhymes (ABAB) are more difficult to model because of longer-range dependencies, but they are still learnable from synthetic data, reaching 45{\%} of rhyming lines in generated samples.",
}

@inproceedings{chakrabarty-etal-2022-help,
    title = "Help me write a Poem: Instruction Tuning as a Vehicle for Collaborative Poetry Writing",
    author = "Chakrabarty, Tuhin  and
      Padmakumar, Vishakh  and
      He, He",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.460",
    doi = "10.18653/v1/2022.emnlp-main.460",
    pages = "6848--6863",
    abstract = "Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of large language models in the realm of computer assisted creativity, in this work, we present \textit{CoPoet}, a collaborative poetry writing system, with the goal of to study if LLM{'}s actually improve the quality of the generated content. In contrast to auto-completing a user{'}s text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as \textit{Write a sentence about {`}love{'}} or \textit{Write a sentence ending in {`}fly{'}}. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive to publicly available LLMs trained on instructions (InstructGPT), but also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from \textit{Monarchy} to \textit{Climate change}, which are preferred by third-party evaluators over poems written without the system.",
}

@inproceedings{tian-peng-2022-zero,
    title = "Zero-shot Sonnet Generation with Discourse-level Planning and Aesthetics Features",
    author = "Tian, Yufei  and
      Peng, Nanyun",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.262",
    doi = "10.18653/v1/2022.naacl-main.262",
    pages = "3587--3597",
    abstract = "Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.",
}

@inproceedings{ormazabal-etal-2022-poelm,
    title = "{P}oe{LM}: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation",
    author = "Ormazabal, Aitor  and
      Artetxe, Mikel  and
      Agirrezabal, Manex  and
      Soroa, Aitor  and
      Agirre, Eneko",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.268",
    doi = "10.18653/v1/2022.findings-emnlp.268",
    pages = "3655--3670",
    abstract = "Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems that follow any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. The transformer learns to link the structure descriptor with the control codes to the number of lines, their length and their end rhyme. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans.",
}

@inproceedings{belouadi_bygpt5_2023,
	location = {Toronto, Canada},
	title = {{ByGPT}5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models},
	url = {https://aclanthology.org/2023.acl-long.406},
	doi = {10.18653/v1/2023.acl-long.406},
	abstract = {State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train {ByGPT}5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that {ByGPT}5 outperforms other models such as {mT}5, {ByT}5, {GPT}-2 and {ChatGPT}, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.},
	pages = {7364--7381},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Belouadi, Jonas and Eger, Steffen},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	date = {2023-07},
}



@article{Pavlik2023Collaborating,title={Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education},author={J. Pavlik},journal={Journalism \& Mass Communication Educator},year={2023},volume={78},pages={84 - 93},doi={10.1177/10776958221149577}}

@inproceedings{wang_neural_2021,
	title = {Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation},
	url = {https://www.semanticscholar.org/paper/Neural-Rule-Execution-Tracking-Machine-For-Text-Wang-Xu/4159a98819c10c2710f68d239fcc03bdb7ece472},
	abstract = {Sequence-to-Sequence (S2S) neural text generation models, especially the pre-trained ones (e.g., {BART} and T5), have exhibited compelling performance on various natural language generation tasks. However, the black-box nature of these models limits their application in tasks where specific rules (e.g., controllable constraints, prior knowledge) need to be executed. Previous works either design specific model structure (e.g., Copy Mechanism corresponding to the rule"the generated output should include certain words in the source input") or implement specialized inference algorithm (e.g., Constrained Beam Search) to execute particular rules through the text generation. These methods require careful design case-by-case and are difficult to support multiple rules concurrently. In this paper, we propose a novel module named Neural Rule-Execution Tracking Machine that can be equipped into various transformer-based generators to leverage multiple rules simultaneously to guide the neural generation model for superior generation performance in a unified and scalable way. Extensive experimental results on several benchmarks verify the effectiveness of our proposed model in both controllable and general text generation.},
	eventtitle = {Neural Information Processing Systems},
	author = {Wang, Yufei and Xu, Can and Hu, Huang and Tao, Chongyang and Wan, Stephen and Dras, M. and Johnson, Mark and Jiang, Daxin},
	urldate = {2024-01-15},
	date = {2021-07-28},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/6Z3J2MZ8/Wang e.a. - 2021 - Neural Rule-Execution Tracking Machine For Transfo.pdf:application/pdf},
}


@article{mudgal_controlled_2023,
	title = {Controlled Decoding from Language Models},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2310.17022},
	doi = {10.48550/ARXIV.2310.17022},
	abstract = {We propose controlled decoding ({CD}), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. {CD} solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that {CD} is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of {CD} makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that {CD} can be applied in a novel blockwise fashion at inference-time, again without the need for any training-time changes, essentially bridging the gap between the popular best-of-\$K\$ strategy and token-level reinforcement learning. This makes {CD} a promising approach for alignment of language models.},
	author = {Mudgal, Sidharth and Lee, Jong and Ganapathy, Harish and Li, {YaGuang} and Wang, Tao and Huang, Yanping and Chen, Zhifeng and Cheng, Heng-Tze and Collins, Michael and Strohman, Trevor and Chen, Jilin and Beutel, Alex and Beirami, Ahmad},
	urldate = {2024-01-15},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG})},
	file = {Full Text PDF:/Users/antonlintermans/Zotero/storage/T5UPUMMI/Mudgal e.a. - 2023 - Controlled Decoding from Language Models.pdf:application/pdf},
}



@misc{krause_gedi_2020,
	title = {{GeDi}: Generative Discriminator Guided Sequence Generation},
	url = {http://arxiv.org/abs/2009.06367},
	doi = {10.48550/arXiv.2009.06367},
	shorttitle = {{GeDi}},
	abstract = {While large-scale language models ({LMs}) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large {LMs} usually contain significant toxicity, hate, bias, and negativity. We propose {GeDi} as an efficient method for using smaller {LMs} as generative discriminators to guide generation from large {LMs} to make them safer and more controllable. {GeDi} guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that {GeDi} gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training {GeDi} on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that {GeDi} can make {GPT}-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.},
	number = {{arXiv}:2009.06367},
	publisher = {{arXiv}},
	author = {Krause, Ben and Gotmare, Akhilesh Deepak and {McCann}, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
	urldate = {2024-01-08},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2009.06367 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/MDQGT3FB/Krause e.a. - 2020 - GeDi Generative Discriminator Guided Sequence Gen.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/V242H25K/2009.html:text/html},
}



@misc{keskar_ctrl_2019,
	title = {{CTRL}: A Conditional Transformer Language Model for Controllable Generation},
	url = {http://arxiv.org/abs/1909.05858},
	shorttitle = {{CTRL}},
	abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release {CTRL}, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow {CTRL} to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of {CTRL} at https://github.com/salesforce/ctrl.},
	number = {{arXiv}:1909.05858},
	publisher = {{arXiv}},
	author = {Keskar, Nitish Shirish and {McCann}, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
	urldate = {2024-01-08},
	date = {2019-09-20},
	eprinttype = {arxiv},
	eprint = {1909.05858 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/9B8NIXY6/1909.html:text/html;Full Text PDF:/Users/antonlintermans/Zotero/storage/Y4E29FLG/Keskar e.a. - 2019 - CTRL A Conditional Transformer Language Model for.pdf:application/pdf},
}

@misc{liu_dexperts_2021,
	title = {{DExperts}: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
	url = {http://arxiv.org/abs/2105.03023},
	doi = {10.48550/arXiv.2105.03023},
	shorttitle = {{DExperts}},
	abstract = {Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose {DExperts}: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with "expert" {LMs} and/or "anti-expert" {LMs} in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply {DExperts} to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because {DExperts} operates only on the output of the pretrained {LM}, it is effective with (anti-)experts of smaller size, including when operating on {GPT}-3. Our work highlights the promise of tuning small {LMs} on text with (un)desirable attributes for efficient decoding-time steering.},
	number = {{arXiv}:2105.03023},
	publisher = {{arXiv}},
	author = {Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A. and Choi, Yejin},
	urldate = {2024-01-08},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2105.03023 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/antonlintermans/Zotero/storage/2LEYQGPI/Liu e.a. - 2021 - DExperts Decoding-Time Controlled Text Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/antonlintermans/Zotero/storage/QPAIZQTZ/2105.html:text/html},
}
