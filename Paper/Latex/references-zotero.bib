
@misc{gemini_team_gemini_2023,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Petrov, Slav and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Chadwick, Martin and Tomar, Gaurav Singh and Garcia, Xavier and Senter, Evan and Taropa, Emanuel and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Zhang, Yujing and Addanki, Ravi and Miech, Antoine and Louis, Annie and Shafey, Laurent El and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Attaluri, Nithya and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaly and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Lin, Hanzhao and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Cheng, Yong and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Li, YaGuang and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Elsayed, Gamaleldin and Chi, Ed and Mahdieh, Mahdis and Tenney, Ian and Hua, Nan and Petrychenko, Ivan and Kane, Patrick and Scandinaro, Dylan and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Sadovsky, Adam and Bunyan, Oskar and Rabiej, Dominik and Wu, Shimu and Zhang, John and Vasudevan, Gautam and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Chan, Betty and Rabinovitch, Pam G. and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Potluri, Sahitya and Park, Jane and Davoodi, Elnaz and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Gorgolewski, Chris and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Suganthan, Paul and Palmer, Evan and Irving, Geoffrey and Loper, Edward and Faruqui, Manaal and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Fink, Michael and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Georgiev, Marin and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Lui, Minnie and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Thiet, Lam Nguyen and Andor, Daniel and Valenzuela, Pedro and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Velury, Sarmishta and Krause, Sebastian and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Latkar, Tejasi and Zhang, Mingyang and Le, Quoc and Abellan, Elena Allica and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Lall, Sid and Franko, Ken and Filonov, Egor and Bulanova, Anna and Leblond, Rémi and Yadav, Vikas and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Zhou, Hao and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Liu, Jeremiah and Omernick, Mark and Bishop, Colton and Kumar, Chintu and Sterneck, Rachel and Foley, Ryan and Jain, Rohan and Mishra, Swaroop and Xia, Jiawei and Bos, Taylor and Cideron, Geoffrey and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Gurita, Petru and Noga, Hila and Shah, Premal and Mankowitz, Daniel J. and Polozov, Alex and Kushman, Nate and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Mohananey, Anhad and Geist, Matthieu and Mudgal, Sidharth and Girgin, Sertan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Yuan, Quan and Bagri, Sumit and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Severyn, Aliaksei and Lai, Jonathan and Wu, Kathy and Cheng, Heng-Tze and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Geller, Mark and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Sozanschi, Andrei and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Goyal, Abhimanyu and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Sabaer and Wieting, John and Ajmeri, Omar and Uria, Benigno and Zhu, Tao and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Tran, Dustin and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Kalb, Norbert and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Lakshminarayanan, Balaji and Deck, Charlie and Upadhyay, Shyam and Lee, Hyo and Dusenberry, Mike and Li, Zonglin and Wang, Xuezhi and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Yue, Summer and Arora, Sho and Malmi, Eric and Mirylenka, Daniil and Tan, Qijun and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Zheng, Steven and Pongetti, Francesco and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Kotikalapudi, Ragha and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Kuang, Chenkai and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Sun, Pei and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Dasgupta, Ishita and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Liu, Yuan and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Penchev, Ivo and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Kurzrok, Adam and Webb, Lynette and Dua, Sahil and Li, Dong and Lahoti, Preethi and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Bilal, Taylan and Eltyshev, Evgenii and Balle, Daniel and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Yu, Adams and Angermueller, Christof and Li, Xiaowei and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Brooks, Kevin and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Jalan, Komal and Li, Dinghua and Perng, Ginger and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Chen, Mia and Milan, Kieran and Mikulik, Vladimir and Strohman, Trevor and Franco, Juliana and Green, Tim and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{chakrabarty_help_2022,
	title = {Help me write a poem: {Instruction} {Tuning} as a {Vehicle} for {Collaborative} {Poetry} {Writing}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Help me write a poem},
	url = {https://arxiv.org/abs/2210.13669},
	doi = {10.48550/ARXIV.2210.13669},
	abstract = {Recent work in training large language models (LLMs) to follow natural language instructions has opened up exciting opportunities for natural language interface design. Building on the prior success of LLMs in the realm of computer-assisted creativity, we aim to study if LLMs can improve the quality of user-generated content through collaboration. We present CoPoet, a collaborative poetry writing system. In contrast to auto-completing a user's text, CoPoet is controlled by user instructions that specify the attributes of the desired text, such as Write a sentence about `love' or Write a sentence ending in `fly'. The core component of our system is a language model fine-tuned on a diverse collection of instructions for poetry writing. Our model is not only competitive with publicly available LLMs trained on instructions (InstructGPT), but is also capable of satisfying unseen compositional instructions. A study with 15 qualified crowdworkers shows that users successfully write poems with CoPoet on diverse topics ranging from Monarchy to Climate change. Further, the collaboratively written poems are preferred by third-party evaluators over those written without the system.},
	urldate = {2024-01-25},
	author = {Chakrabarty, Tuhin and Padmakumar, Vishakh and He, He},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@misc{kumar_controlled_2021,
	title = {Controlled {Text} {Generation} as {Continuous} {Optimization} with {Multiple} {Constraints}},
	url = {http://arxiv.org/abs/2108.01850},
	doi = {10.48550/arXiv.2108.01850},
	abstract = {As large-scale language model pretraining pushes the state-of-the-art in text generation, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via fine-tuning remains the popular approach, it incurs a significant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MuCoCO -- a flexible and modular algorithm for controllable inference from pretrained models. We formulate the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe significant improvements over baselines.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {Kumar, Sachin and Malmi, Eric and Severyn, Aliaksei and Tsvetkov, Yulia},
	month = aug,
	year = {2021},
	note = {arXiv:2108.01850 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{mireshghallah_mix_2022,
	title = {Mix and {Match}: {Learning}-free {Controllable} {Text} {Generation} using {Energy} {Language} {Models}},
	shorttitle = {Mix and {Match}},
	url = {http://arxiv.org/abs/2203.13299},
	doi = {10.48550/arXiv.2203.13299},
	abstract = {Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.},
	urldate = {2024-01-22},
	publisher = {arXiv},
	author = {Mireshghallah, Fatemehsadat and Goyal, Kartik and Berg-Kirkpatrick, Taylor},
	month = apr,
	year = {2022},
	note = {arXiv:2203.13299 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{qin_cold_2022,
	title = {{COLD} {Decoding}: {Energy}-based {Constrained} {Text} {Generation} with {Langevin} {Dynamics}},
	shorttitle = {{COLD} {Decoding}},
	url = {https://openreview.net/forum?id=TiZYrQ-mPup},
	abstract = {Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.},
	language = {en},
	urldate = {2024-01-19},
	author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
	month = may,
	year = {2022},
}

@misc{chan_cocon_2022,
	title = {{CoCon}: {A} {Self}-{Supervised} {Approach} for {Controlled} {Text} {Generation}},
	shorttitle = {{CoCon}},
	url = {http://arxiv.org/abs/2006.03535},
	doi = {10.48550/arXiv.2006.03535},
	abstract = {Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word- and phrase-level. Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a content input, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Chan, Alvin and Ong, Yew-Soon and Pung, Bill and Zhang, Aston and Fu, Jie},
	month = jun,
	year = {2022},
	note = {arXiv:2006.03535 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{khalifa_distributional_2021,
	title = {A {Distributional} {Approach} to {Controlled} {Text} {Generation}},
	url = {http://arxiv.org/abs/2012.11635},
	doi = {10.48550/arXiv.2012.11635},
	abstract = {We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both "pointwise" and "distributional" constraints over the target LM -- to our knowledge, the first model with such generality -- while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence. (Code available at https://github.com/naver/gdc)},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Khalifa, Muhammad and Elsahar, Hady and Dymetman, Marc},
	month = may,
	year = {2021},
	note = {arXiv:2012.11635 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{chen_relation-constrained_2022,
	title = {Relation-{Constrained} {Decoding} for {Text} {Generation}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/ab63a1a325670278ba9b87fbc3e95e33-Abstract-Conference.html},
	language = {en},
	urldate = {2024-01-18},
	journal = {Advances in Neural Information Processing Systems},
	author = {Chen, Xiang and Yang, Zhixian and Wan, Xiaojun},
	month = dec,
	year = {2022},
	pages = {26804--26819},
}

@misc{qin_back_2021,
	title = {Back to the {Future}: {Unsupervised} {Backprop}-based {Decoding} for {Counterfactual} and {Abductive} {Commonsense} {Reasoning}},
	shorttitle = {Back to the {Future}},
	url = {http://arxiv.org/abs/2010.05906},
	doi = {10.48550/arXiv.2010.05906},
	abstract = {Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future. However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision. The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters. By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts. We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Qin, Lianhui and Shwartz, Vered and West, Peter and Bhagavatula, Chandra and Hwang, Jena and Bras, Ronan Le and Bosselut, Antoine and Choi, Yejin},
	month = aug,
	year = {2021},
	note = {arXiv:2010.05906 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{west_reflective_2021,
	address = {Online},
	title = {Reflective {Decoding}: {Beyond} {Unidirectional} {Generation} with {Off}-the-{Shelf} {Language} {Models}},
	shorttitle = {Reflective {Decoding}},
	url = {https://aclanthology.org/2021.acl-long.114},
	doi = {10.18653/v1/2021.acl-long.114},
	abstract = {Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right. As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision. In this paper, we present Reflective Decoding, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks. Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward. First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing). Second, in the reflection step, we condition on these “context ensembles”, generating outputs that are compatible with them. Comprehensive empirical results demonstrate that Reflective Decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods. Reflective Decoding surpasses multiple supervised baselines on various metrics including human evaluation.},
	urldate = {2024-01-16},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {West, Peter and Lu, Ximing and Holtzman, Ari and Bhagavatula, Chandra and Hwang, Jena D. and Choi, Yejin},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	pages = {1435--1450},
}

@misc{leblond_machine_2021,
	title = {Machine {Translation} {Decoding} beyond {Beam} {Search}},
	url = {http://arxiv.org/abs/2104.05336},
	doi = {10.48550/arXiv.2104.05336},
	abstract = {Beam search is the go-to method for decoding auto-regressive machine translation models. While it yields consistent improvements in terms of BLEU, it is only concerned with finding outputs with high model likelihood, and is thus agnostic to whatever end metric or score practitioners care about. Our aim is to establish whether beam search can be replaced by a more powerful metric-driven search technique. To this end, we explore numerous decoding algorithms, including some which rely on a value function parameterised by a neural network, and report results on a variety of metrics. Notably, we introduce a Monte-Carlo Tree Search (MCTS) based method and showcase its competitiveness. We provide a blueprint for how to use MCTS fruitfully in language applications, which opens promising future directions. We find that which algorithm is best heavily depends on the characteristics of the goal metric; we believe that our extensive experiments and analysis will inform further research in this area.},
	urldate = {2024-01-16},
	publisher = {arXiv},
	author = {Leblond, Rémi and Alayrac, Jean-Baptiste and Sifre, Laurent and Pislar, Miruna and Lespiau, Jean-Baptiste and Antonoglou, Ioannis and Simonyan, Karen and Vinyals, Oriol},
	month = apr,
	year = {2021},
	note = {arXiv:2104.05336 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{zheng_toward_2023,
	title = {Toward {Unified} {Controllable} {Text} {Generation} via {Regular} {Expression} {Instruction}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.10447},
	doi = {10.48550/ARXIV.2309.10447},
	abstract = {Controllable text generation is a fundamental aspect of natural language generation, with numerous methods proposed for different constraint types. However, these approaches often require significant architectural or decoding modifications, making them challenging to apply to additional constraints or resolve different constraint combinations. To address this, our paper introduces Regular Expression Instruction (REI), which utilizes an instruction-based mechanism to fully exploit regular expressions' advantages to uniformly model diverse constraints. Specifically, our REI supports all popular fine-grained controllable generation constraints, i.e., lexical, positional, and length, as well as their complex combinations, via regular expression-style instructions. Our method only requires fine-tuning on medium-scale language models or few-shot, in-context learning on large language models, and requires no further adjustment when applied to various constraint combinations. Experiments demonstrate that our straightforward approach yields high success rates and adaptability to various constraints while maintaining competitiveness in automatic metrics and outperforming most previous baselines.},
	urldate = {2024-01-15},
	author = {Zheng, Xin and Lin, Hongyu and Han, Xianpei and Sun, Le},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{zhang_controllable_2023,
	title = {Controllable {Text} {Generation} with {Residual} {Memory} {Transformer}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.16231},
	doi = {10.48550/ARXIV.2309.16231},
	abstract = {Large-scale Causal Language Models (CLMs), e.g., GPT3 and ChatGPT, have brought great success in text generation. However, it is still an open challenge to control the generation process of CLM while balancing flexibility, control granularity, and generation efficiency. In this paper, we provide a new alternative for controllable text generation (CTG), by designing a non-intrusive, lightweight control plugin to accompany the generation of CLM at arbitrary time steps. The proposed control plugin, namely Residual Memory Transformer (RMT), has an encoder-decoder setup, which can accept any types of control conditions and cooperate with CLM through a residual learning paradigm, to achieve a more flexible, general, and efficient CTG. Extensive experiments are carried out on various control tasks, in the form of both automatic and human evaluations. The results show the superiority of RMT over a range of state-of-the-art approaches, proving the effectiveness and versatility of our approach.},
	urldate = {2024-01-15},
	author = {Zhang, Hanqing and Si, Sun and Wu, Haiming and Song, Dawei},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@inproceedings{bastan_neurostructural_2023,
	address = {Toronto, Canada},
	title = {{NEUROSTRUCTURAL} {DECODING}: {Neural} {Text} {Generation} with {Structural} {Constraints}},
	shorttitle = {{NEUROSTRUCTURAL} {DECODING}},
	url = {https://aclanthology.org/2023.acl-long.528},
	doi = {10.18653/v1/2023.acl-long.528},
	abstract = {Text generation often involves producing coherent and grammatically correct texts that also satisfy a given set of semantic constraints. While most approaches for conditional text generation have primarily focused on lexical constraints, they often struggle to effectively incorporate syntactic constraints, which provide a richer language for approximating semantic constraints.We address this gap by introducing NeuroStructural Decoding, a new decoding algorithm that incorporates syntactic constraints to further improve the quality of the generated text. We build NeuroStructural Decoding on the NeuroLogic Decoding (Lu etal. 2021) algorithm, which enables language generation models to produce fluent text while satisfying complex lexical constraints. Our algorithm is powerful and scalable. It tracks lexico-syntactic constraints (e.g., we need to observe dog as subject and ball as object)during decoding by parsing the partial generations at each step. To this end, we adapt a dependency parser to generate parses for incomplete sentences. Our approach is evaluated on three different language generation tasks, and the results show improved performance in both lexical and syntactic metrics compared to previous methods. The results suggest this is a promising solution for integrating fine-grained controllable generation into the conventional beam search decoding.},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bastan, Mohaddeseh and Surdeanu, Mihai and Balasubramanian, Niranjan},
	year = {2023},
	pages = {9496--9510},
}

@inproceedings{xu_best-k_2022,
	title = {Best-\$k\$ {Search} {Algorithm} for {Neural} {Text} {Generation}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2211.11924},
	doi = {10.48550/ARXIV.2211.11924},
	abstract = {Modern natural language generation paradigms require a good decoding strategy to obtain quality sequences out of the model. Beam search yields high-quality but low diversity outputs; stochastic approaches suffer from high variance and sometimes low quality, but the outputs tend to be more natural and creative. In this work, we propose a deterministic search algorithm balancing both quality and diversity. We first investigate the vanilla best-first search (BFS) algorithm and then propose the Best-\$k\$ Search algorithm. Inspired by BFS, we greedily expand the top \$k\$ nodes, instead of only the first node, to boost efficiency and diversity. Upweighting recently discovered nodes accompanied by heap pruning ensures the completeness of the search procedure. Experiments on four NLG tasks, including question generation, commonsense generation, text summarization, and translation, show that best-\$k\$ search yields more diverse and natural outputs compared to strong baselines, while our approach maintains high text quality. The proposed algorithm is parameter-free, lightweight, efficient, and easy to use.},
	urldate = {2024-01-15},
	publisher = {arXiv},
	author = {Xu, Jiacheng and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
	year = {2022},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
}

@article{zhang_planning_2023,
	title = {Planning with {Large} {Language} {Models} for {Code} {Generation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.05510},
	doi = {10.48550/ARXIV.2303.05510},
	abstract = {Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner to generate candidate programs and test them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.},
	urldate = {2024-01-15},
	author = {Zhang, Shun and Chen, Zhenfang and Shen, Yikang and Ding, Mingyu and Tenenbaum, Joshua B. and Gan, Chuang},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG), Programming Languages (cs.PL)},
}

@article{zhou_controlled_2023,
	title = {Controlled {Text} {Generation} with {Natural} {Language} {Instructions}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2304.14293},
	doi = {10.48550/ARXIV.2304.14293},
	abstract = {Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCTG is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, InstructCTG allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.},
	urldate = {2024-01-15},
	author = {Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Wilcox, Ethan and Cotterell, Ryan and Sachan, Mrinmaya},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{bonlarron_constraints_2023,
	address = {Macau, SAR China},
	title = {Constraints {First}: {A} {New} {MDD}-based {Model} to {Generate} {Sentences} {Under} {Constraints}},
	isbn = {978-1-956792-03-4},
	shorttitle = {Constraints {First}},
	url = {https://www.ijcai.org/proceedings/2023/210},
	doi = {10.24963/ijcai.2023/210},
	abstract = {This paper introduces a new approach to generating strongly constrained texts. We consider standardized sentence generation for the typical application of vision screening. To solve this problem, we formalize it as a discrete combinatorial optimization problem and utilize multivalued decision diagrams (MDD), a well-known data structure to deal with constraints. In our context, one key strength of MDD is to compute an exhaustive set of solutions without performing any search. Once the sentences are obtained, we apply a language model (GPT-2) to keep the best ones. We detail this for English and also for French where the agreement and conjugation rules are known to be more complex. Finally, with the help of GPT-2, we get hundreds of bona-fide candidate sentences. When compared with the few dozen sentences usually available in the well-known vision screening test (MNREAD), this brings a major breakthrough in the field of standardized sentence generation. Also, as it can be easily adapted for other languages, it has the potential to make the MNREAD test even more valuable and usable. More generally, this paper highlights MDD as a convincing alternative for constrained text generation, especially when the constraints are hard to satisfy, but also for many other prospects.},
	language = {en},
	urldate = {2024-01-15},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Bonlarron, Alexandre and Calabrèse, Aurélie and Kornprobst, Pierre and Régin, Jean-Charles},
	month = aug,
	year = {2023},
	pages = {1893--1901},
}

@article{mudgal_controlled_2023,
	title = {Controlled {Decoding} from {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2310.17022},
	doi = {10.48550/ARXIV.2310.17022},
	abstract = {We propose controlled decoding (CD), a novel off-policy reinforcement learning method to control the autoregressive generation from language models towards high reward outcomes. CD solves an off-policy reinforcement learning problem through a value function for the reward, which we call a prefix scorer. The prefix scorer is used at inference time to steer the generation towards higher reward outcomes. We show that the prefix scorer may be trained on (possibly) off-policy data to predict the expected reward when decoding is continued from a partially decoded response. We empirically demonstrate that CD is effective as a control mechanism on Reddit conversations corpus. We also show that the modularity of the design of CD makes it possible to control for multiple rewards, effectively solving a multi-objective reinforcement learning problem with no additional complexity. Finally, we show that CD can be applied in a novel blockwise fashion at inference-time, again without the need for any training-time changes, essentially bridging the gap between the popular best-of-\$K\$ strategy and token-level reinforcement learning. This makes CD a promising approach for alignment of language models.},
	urldate = {2024-01-15},
	author = {Mudgal, Sidharth and Lee, Jong and Ganapathy, Harish and Li, YaGuang and Wang, Tao and Huang, Yanping and Chen, Zhifeng and Cheng, Heng-Tze and Collins, Michael and Strohman, Trevor and Chen, Jilin and Beutel, Alex and Beirami, Ahmad},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@inproceedings{chen_relation-constrained_2022-1,
	title = {Relation-{Constrained} {Decoding} for {Text} {Generation}},
	url = {https://www.semanticscholar.org/paper/Relation-Constrained-Decoding-for-Text-Generation-Chen-Yang/f55b57e078f852ac200245689835783ecf07f2de},
	abstract = {The dominant paradigm for neural text generation nowadays is seq2seq learning with large-scale pretrained language models. However, it is usually difficult to manually constrain the generation process of these models. Prior studies have introduced Lexically Constrained Decoding (LCD) to ensure the presence of pre-specified words or phrases in the output. However, simply applying lexical constraints has no guarantee of the grammatical or semantic relations between words. Thus, more elaborate constraints are needed. To this end, we first propose a new constrained decoding scenario named Relation-Constrained Decoding (RCD) , which requires the model’s output to contain several given word pairs with respect to the given relations between them. For this scenario, we present a novel plug-and-play decoding algorithm named RE lation-guided probability S urgery and b E am AL location (RESEAL), which can handle different categories of relations, e.g., syn-tactical relations or factual relations. Moreover, RESEAL can adaptively “reseal” the relations to form a high-quality sentence, which can be applied to the inference stage of any autoregressive text generation model. To evaluate our method, we first construct an RCD benchmark based on dependency relations from treebanks with annotated dependencies. Experimental results demonstrate that our approach can achieve better preservation of the input dependency relations compared to previous methods. To further illustrate the effectiveness of RESEAL, we apply our method to three downstream tasks: sentence summarization, fact-based text editing, and data-to-text generation. We observe an improvement in generation quality. The source code is available at https://github.com/CasparSwift/RESEAL .},
	urldate = {2024-01-15},
	author = {Chen, X. and Yang, Zhixian and Wan, Xiaojun},
	year = {2022},
}

@inproceedings{wang_neural_2021,
	title = {Neural {Rule}-{Execution} {Tracking} {Machine} {For} {Transformer}-{Based} {Text} {Generation}},
	url = {https://www.semanticscholar.org/paper/Neural-Rule-Execution-Tracking-Machine-For-Text-Wang-Xu/4159a98819c10c2710f68d239fcc03bdb7ece472},
	abstract = {Sequence-to-Sequence (S2S) neural text generation models, especially the pre-trained ones (e.g., BART and T5), have exhibited compelling performance on various natural language generation tasks. However, the black-box nature of these models limits their application in tasks where specific rules (e.g., controllable constraints, prior knowledge) need to be executed. Previous works either design specific model structure (e.g., Copy Mechanism corresponding to the rule"the generated output should include certain words in the source input") or implement specialized inference algorithm (e.g., Constrained Beam Search) to execute particular rules through the text generation. These methods require careful design case-by-case and are difficult to support multiple rules concurrently. In this paper, we propose a novel module named Neural Rule-Execution Tracking Machine that can be equipped into various transformer-based generators to leverage multiple rules simultaneously to guide the neural generation model for superior generation performance in a unified and scalable way. Extensive experimental results on several benchmarks verify the effectiveness of our proposed model in both controllable and general text generation.},
	urldate = {2024-01-15},
	author = {Wang, Yufei and Xu, Can and Hu, Huang and Tao, Chongyang and Wan, Stephen and Dras, M. and Johnson, Mark and Jiang, Daxin},
	month = jul,
	year = {2021},
}

@article{lu_bounding_2023,
	title = {Bounding the {Capabilities} of {Large} {Language} {Models} in {Open} {Text} {Generation} with {Prompt} {Constraints}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2302.09185},
	doi = {10.48550/ARXIV.2302.09185},
	abstract = {The limits of open-ended generative models are unclear, yet increasingly important. What causes them to succeed and what causes them to fail? In this paper, we take a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models. We present a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic. These constraint types are categorized into a set of well-defined constraints that are analyzable by a single prompt. We then systematically create a diverse set of simple, natural, and useful prompts to robustly analyze each individual constraint. Using the GPT-3 text-davinci-002 model as a case study, we generate outputs from our collection of prompts and analyze the model's generative failures. We also show the generalizability of our proposed method on other large models like BLOOM and OPT. Our results and our in-context mitigation strategies reveal open challenges for future research. We have publicly released our code at https://github.com/SALT-NLP/Bound-Cap-LLM.},
	urldate = {2024-01-15},
	author = {Lu, Albert and Zhang, Hongxin and Zhang, Yanzhe and Wang, Xuezhi and Yang, Diyi},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
}

@misc{chaffin_ppl-mcts_2022,
	title = {{PPL}-{MCTS}: {Constrained} {Textual} {Generation} {Through} {Discriminator}-{Guided} {MCTS} {Decoding}},
	shorttitle = {{PPL}-{MCTS}},
	url = {http://arxiv.org/abs/2109.13582},
	doi = {10.48550/arXiv.2109.13582},
	abstract = {Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM. Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically. We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.},
	urldate = {2024-01-11},
	publisher = {arXiv},
	author = {Chaffin, Antoine and Claveau, Vincent and Kijak, Ewa},
	month = may,
	year = {2022},
	note = {arXiv:2109.13582 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{liu_dexperts_2021,
	title = {{DExperts}: {Decoding}-{Time} {Controlled} {Text} {Generation} with {Experts} and {Anti}-{Experts}},
	shorttitle = {{DExperts}},
	url = {http://arxiv.org/abs/2105.03023},
	doi = {10.48550/arXiv.2105.03023},
	abstract = {Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with "expert" LMs and/or "anti-expert" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta, Swabha and Bhagavatula, Chandra and Smith, Noah A. and Choi, Yejin},
	month = jun,
	year = {2021},
	note = {arXiv:2105.03023 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{krause_gedi_2020,
	title = {{GeDi}: {Generative} {Discriminator} {Guided} {Sequence} {Generation}},
	shorttitle = {{GeDi}},
	url = {http://arxiv.org/abs/2009.06367},
	doi = {10.48550/arXiv.2009.06367},
	abstract = {While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Krause, Ben and Gotmare, Akhilesh Deepak and McCann, Bryan and Keskar, Nitish Shirish and Joty, Shafiq and Socher, Richard and Rajani, Nazneen Fatema},
	month = oct,
	year = {2020},
	note = {arXiv:2009.06367 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dathathri_plug_2020,
	title = {Plug and {Play} {Language} {Models}: {A} {Simple} {Approach} to {Controlled} {Text} {Generation}},
	shorttitle = {Plug and {Play} {Language} {Models}},
	url = {http://arxiv.org/abs/1912.02164},
	doi = {10.48550/arXiv.1912.02164},
	abstract = {Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
	month = mar,
	year = {2020},
	note = {arXiv:1912.02164 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{keskar_ctrl_2019,
	title = {{CTRL}: {A} {Conditional} {Transformer} {Language} {Model} for {Controllable} {Generation}},
	shorttitle = {{CTRL}},
	url = {http://arxiv.org/abs/1909.05858},
	abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
	urldate = {2024-01-08},
	publisher = {arXiv},
	author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
	month = sep,
	year = {2019},
	note = {arXiv:1909.05858 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{peng_towards_2018,
	address = {New Orleans, Louisiana},
	title = {Towards {Controllable} {Story} {Generation}},
	url = {https://aclanthology.org/W18-1505},
	doi = {10.18653/v1/W18-1505},
	abstract = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
	urldate = {2024-01-08},
	booktitle = {Proceedings of the {First} {Workshop} on {Storytelling}},
	publisher = {Association for Computational Linguistics},
	author = {Peng, Nanyun and Ghazvininejad, Marjan and May, Jonathan and Knight, Kevin},
	editor = {Mitchell, Margaret and Huang, Ting-Hao `Kenneth' and Ferraro, Francis and Misra, Ishan},
	month = jun,
	year = {2018},
	pages = {43--49},
}

@article{liao_gpt-based_2019,
	title = {{GPT}-based {Generation} for {Classical} {Chinese} {Poetry}},
	url = {https://www.semanticscholar.org/paper/GPT-based-Generation-for-Classical-Chinese-Poetry-Liao-Wang/83b56c3c7a61767bd88d85796aa5dbc4976912c3},
	abstract = {We present a simple yet effective method for generating high quality classical Chinese poetry with Generative Pre-trained Language Model (GPT). The method adopts a simple GPT model, without using any human crafted rules or features, or designing any additional neural components. While the proposed model learns to generate various forms of classical Chinese poems, including Jueju, Lushi, various Cipai and Couples, the generated poems are of very high quality. We also propose and implement a method to fine-tune the model to generate acrostic poetry. To the best of our knowledge, this is the first to employ GPT in developing a poetry generation system. We have released an online mini demonstration program on Wechat to show the generation capability of the proposed method for classical Chinese poetry.},
	urldate = {2024-01-06},
	journal = {ArXiv},
	author = {Liao, Yi and Wang, Yasheng and Liu, Qun and Jiang, Xin},
	month = jun,
	year = {2019},
}

@inproceedings{li_rigid_2020,
	address = {Online},
	title = {Rigid {Formats} {Controlled} {Text} {Generation}},
	url = {https://aclanthology.org/2020.acl-main.68},
	doi = {10.18653/v1/2020.acl-main.68},
	abstract = {Neural text generation has made tremendous progress in various tasks. One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating. However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc. The typical characteristics of these texts are in three folds: (1) They must comply fully with the rigid predefined formats. (2) They must obey some rhyming schemes. (3) Although they are restricted to some formats, the sentence integrity must be guaranteed. To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated. Therefore, we propose a simple and elegant framework named SongNet to tackle this problem. The backbone of the framework is a Transformer-based auto-regressive language model. Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity. We improve the attention mechanism to impel the model to capture some future information on the format. A pre-training and fine-tuning framework is designed to further improve the generation quality. Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Piji and Zhang, Haisong and Liu, Xiaojiang and Shi, Shuming},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	pages = {742--751},
}

@inproceedings{tian_zero-shot_2022,
	address = {Seattle, United States},
	title = {Zero-shot {Sonnet} {Generation} with {Discourse}-level {Planning} and {Aesthetics} {Features}},
	url = {https://aclanthology.org/2022.naacl-main.262},
	doi = {10.18653/v1/2022.naacl-main.262},
	abstract = {Poetry generation, and creative language generation in general, usually suffers from the lack of large training data. In this paper, we present a novel framework to generate sonnets that does not require training on poems. We design a hierarchical framework which plans the poem sketch before decoding. Specifically, a content planning module is trained on non-poetic texts to obtain discourse-level coherence; then a rhyme module generates rhyme words and a polishing module introduces imagery and similes for aesthetics purposes. Finally, we design a constrained decoding algorithm to impose the meter-and-rhyme constraint of the generated sonnets. Automatic and human evaluation show that our multi-stage approach without training on poem corpora generates more coherent, poetic, and creative sonnets than several strong baselines.},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tian, Yufei and Peng, Nanyun},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	pages = {3587--3597},
}

@misc{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	doi = {10.48550/arXiv.1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv:1904.09751 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vijayakumar_diverse_2018,
	title = {Diverse {Beam} {Search}: {Decoding} {Diverse} {Solutions} from {Neural} {Sequence} {Models}},
	shorttitle = {Diverse {Beam} {Search}},
	url = {http://arxiv.org/abs/1610.02424},
	doi = {10.48550/arXiv.1610.02424},
	abstract = {Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates - resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space - implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory over- head as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Further, we study the role of diversity for image-grounded language generation tasks as the complexity of the image changes. We observe that our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Vijayakumar, Ashwin K. and Cogswell, Michael and Selvaraju, Ramprasath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
	month = oct,
	year = {2018},
	note = {arXiv:1610.02424 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{hu_improved_2019,
	address = {Minneapolis, Minnesota},
	title = {Improved {Lexically} {Constrained} {Decoding} for {Translation} and {Monolingual} {Rewriting}},
	url = {http://aclweb.org/anthology/N19-1090},
	doi = {10.18653/v1/N19-1090},
	abstract = {Lexically-constrained sequence decoding allows for explicit positive or negative phrase-based constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting. We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three.},
	language = {en},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Hu, J. Edward and Khayrallah, Huda and Culkin, Ryan and Xia, Patrick and Chen, Tongfei and Post, Matt and Van Durme, Benjamin},
	year = {2019},
	pages = {839--850},
}

@article{pascual_directed_2020,
	title = {Directed {Beam} {Search}: {Plug}-and-{Play} {Lexically} {Constrained} {Language} {Generation}},
	shorttitle = {Directed {Beam} {Search}},
	url = {https://www.semanticscholar.org/paper/Directed-Beam-Search%3A-Plug-and-Play-Lexically-Pascual-Egressy/568deb4817e7633483c93be3f50dcf51493963ff},
	abstract = {Large pre-trained language models are capable of generating realistic text. However, controlling these models so that the generated text satisfies lexical constraints, i.e., contains specific words, is a challenging problem. Given that state-of-the-art language models are too large to be trained from scratch in a manageable time, it is desirable to control these models without re-training them. Methods capable of doing this are called plug-and-play. Recent plug-and-play methods have been successful in constraining small bidirectional language models as well as forward models in tasks with a restricted search space, e.g., machine translation. However, controlling large transformer-based models to meet lexical constraints without re-training them remains a challenge. In this work, we propose Directed Beam Search (DBS), a plug-and-play method for lexically constrained language generation. Our method can be applied to any language model, is easy to implement and can be used for general language generation. In our experiments we use DBS to control GPT-2. We demonstrate its performance on keyword-to-phrase generation and we obtain comparable results as a state-of-the-art non-plug-and-play model for lexically constrained story generation.},
	urldate = {2024-01-06},
	journal = {ArXiv},
	author = {Pascual, Damian and Egressy, Béni and Bolli, Florian and Wattenhofer, Roger},
	month = dec,
	year = {2020},
}

@inproceedings{lu_neurologic_2022,
	address = {Seattle, United States},
	title = {{NeuroLogic} {A}*esque {Decoding}: {Constrained} {Text} {Generation} with {Lookahead} {Heuristics}},
	shorttitle = {{NeuroLogic} {A}*esque {Decoding}},
	url = {https://aclanthology.org/2022.naacl-main.57},
	doi = {10.18653/v1/2022.naacl-main.57},
	abstract = {The dominant paradigm for neural text generation is left-to-right decoding from autoregressive language models. Constrained or controllable generation under complex lexical constraints, however, requires foresight to plan ahead feasible future paths. Drawing inspiration from the A{\textasciicircum}* search algorithm, we propose NeuroLogic A*esque, a decoding algorithm that incorporates heuristic estimates of future cost. We develop lookahead heuristics that are efficient for large-scale language models, making our method a drop-in replacement for common techniques such as beam search and top-k sampling. To enable constrained generation, we build on NeuroLogic decoding (Lu et al., 2021), combining its flexibility in incorporating logical constraints with A*esque estimates of future constraint satisfaction. Our approach outperforms competitive baselines on five generation tasks, and achieves new state-of-the-art performance on table-to-text generation, constrained machine translation, and keyword-constrained generation. The improvements are particularly notable on tasks that require complex constraint satisfaction or in few-shot or zero-shot settings. NeuroLogic A*esque illustrates the power of decoding for improving and enabling new capabilities of large-scale language models.},
	language = {en},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Lu, Ximing and Welleck, Sean and West, Peter and Jiang, Liwei and Kasai, Jungo and Khashabi, Daniel and Le Bras, Ronan and Qin, Lianhui and Yu, Youngjae and Zellers, Rowan and Smith, Noah and Choi, Yejin},
	year = {2022},
	pages = {780--799},
}

@inproceedings{hokamp_lexically_2017,
	address = {Vancouver, Canada},
	title = {Lexically {Constrained} {Decoding} for {Sequence} {Generation} {Using} {Grid} {Beam} {Search}},
	url = {http://aclweb.org/anthology/P17-1141},
	doi = {10.18653/v1/P17-1141},
	abstract = {We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model’s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.},
	language = {en},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hokamp, Chris and Liu, Qun},
	year = {2017},
	pages = {1535--1546},
}

@inproceedings{anderson_guided_2017,
	address = {Copenhagen, Denmark},
	title = {Guided {Open} {Vocabulary} {Image} {Captioning} with {Constrained} {Beam} {Search}},
	url = {https://aclanthology.org/D17-1098},
	doi = {10.18653/v1/D17-1098},
	abstract = {Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.},
	urldate = {2024-01-06},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
	editor = {Palmer, Martha and Hwa, Rebecca and Riedel, Sebastian},
	month = sep,
	year = {2017},
	pages = {936--945},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-01-05},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {{GloVe}: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {{GloVe}},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	urldate = {2024-01-05},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	month = oct,
	year = {2014},
	pages = {1532--1543},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {https://arxiv.org/abs/1301.3781v3},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	language = {en},
	urldate = {2024-01-05},
	journal = {arXiv.org},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
}

@misc{tan_progressive_2021,
	title = {Progressive {Generation} of {Long} {Text} with {Pretrained} {Language} {Models}},
	url = {http://arxiv.org/abs/2006.15720},
	abstract = {Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.},
	urldate = {2024-01-03},
	publisher = {arXiv},
	author = {Tan, Bowen and Yang, Zichao and AI-Shedivat, Maruan and Xing, Eric P. and Hu, Zhiting},
	month = apr,
	year = {2021},
	note = {arXiv:2006.15720 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{hazra_saycanpay_2023,
	title = {{SayCanPay}: {Heuristic} {Planning} with {Large} {Language} {Models} using {Learnable} {Domain} {Knowledge}},
	shorttitle = {{SayCanPay}},
	url = {http://arxiv.org/abs/2308.12682},
	doi = {10.48550/arXiv.2308.12682},
	abstract = {Large Language Models (LLMs) have demonstrated impressive planning abilities due to their vast "world knowledge". Yet, obtaining plans that are both feasible (grounded in affordances) and cost-effective (in plan length), remains a challenge, despite recent progress. This contrasts with heuristic planning methods that employ domain knowledge (formalized in action models such as PDDL) and heuristic search to generate feasible, optimal plans. Inspired by this, we propose to combine the power of LLMs and heuristic planning by leveraging the world knowledge of LLMs and the principles of heuristic search. Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay), and heuristic search to select the best sequence of actions. Our contributions are (1) a novel framing of the LLM planning problem in the context of heuristic planning, (2) integrating grounding and cost-effective elements into the generated plans, and (3) using heuristic search over actions. Our extensive evaluations show that our model surpasses other LLM planning approaches.},
	urldate = {2023-12-19},
	publisher = {arXiv},
	author = {Hazra, Rishi and Martires, Pedro Zuidberg Dos and De Raedt, Luc},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12682 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{su_contrastive_2023,
	title = {Contrastive {Search} {Is} {What} {You} {Need} {For} {Neural} {Text} {Generation}},
	url = {http://arxiv.org/abs/2210.14140},
	doi = {10.48550/arXiv.2210.14140},
	abstract = {Generating text with autoregressive language models (LMs) is of great importance to many natural language processing (NLP) applications. Previous solutions for this task often produce text that contains degenerative expressions or lacks semantic consistency. Recently, Su et al. introduced a new decoding method, contrastive search, based on the isotropic representation space of the language model and obtained new state of the art on various benchmarks. Additionally, Su et al. argued that the representations of autoregressive LMs (e.g. GPT-2) are intrinsically anisotropic which is also shared by previous studies. Therefore, to ensure the language model follows an isotropic distribution, Su et al. proposed a contrastive learning scheme, SimCTG, which calibrates the language model's representations through additional training. In this study, we first answer the question: "Are autoregressive LMs really anisotropic?". To this end, we extensively evaluate the isotropy of LMs across 16 major languages. Surprisingly, we find that the anisotropic problem only exists in the two specific English GPT-2-small/medium models. On the other hand, all other evaluated LMs are naturally isotropic which is in contrast to the conclusion drawn by previous studies. Based on our findings, we further assess the contrastive search decoding method using off-the-shelf LMs on four generation tasks across 16 languages. Our experimental results demonstrate that contrastive search significantly outperforms previous decoding methods without any additional training. More notably, on 12 out of the 16 evaluated languages, contrastive search performs comparably with human-level performances as judged by human evaluations. Our code and other related resources are publicly available at https://github.com/yxuansu/Contrastive\_Search\_Is\_What\_You\_Need.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Su, Yixuan and Collier, Nigel},
	month = feb,
	year = {2023},
	note = {arXiv:2210.14140 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{su_contrastive_2022,
	title = {A {Contrastive} {Framework} for {Neural} {Text} {Generation}},
	url = {http://arxiv.org/abs/2202.06417},
	doi = {10.48550/arXiv.2202.06417},
	abstract = {Text generation is of great importance to many natural language processing applications. However, maximization-based decoding methods (e.g. beam search) of neural language models often lead to degenerate solutions -- the generated text is unnatural and contains undesirable repetitions. Existing approaches introduce stochasticity via sampling or modify training objectives to decrease probabilities of certain tokens (e.g., unlikelihood training). However, they often lead to solutions that lack coherence. In this work, we show that an underlying reason for model degeneration is the anisotropic distribution of token representations. We present a contrastive solution: (i) SimCTG, a contrastive training objective to calibrate the model's representation space, and (ii) a decoding method -- contrastive search -- to encourage diversity while maintaining coherence in the generated text. Extensive experiments and analyses on three benchmarks from two languages demonstrate that our proposed approach significantly outperforms current state-of-the-art text generation methods as evaluated by both human and automatic metrics.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Su, Yixuan and Lan, Tian and Wang, Yan and Yogatama, Dani and Kong, Lingpeng and Collier, Nigel},
	month = sep,
	year = {2022},
	note = {arXiv:2202.06417 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{welleck_consistency_2020,
	title = {Consistency of a {Recurrent} {Language} {Model} {With} {Respect} to {Incomplete} {Decoding}},
	url = {http://arxiv.org/abs/2002.02492},
	doi = {10.48550/arXiv.2002.02492},
	abstract = {Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
	month = oct,
	year = {2020},
	note = {arXiv:2002.02492 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{murray_correcting_2018,
	title = {Correcting {Length} {Bias} in {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1808.10006},
	doi = {10.48550/arXiv.1808.10006},
	abstract = {We study two problems in neural machine translation (NMT). First, in beam search, whereas a wider beam should in principle help translation, it often hurts NMT. Second, NMT has a tendency to produce translations that are too short. Here, we argue that these problems are closely related and both rooted in label bias. We show that correcting the brevity problem almost eliminates the beam problem; we compare some commonly-used methods for doing this, finding that a simple per-word reward works well; and we introduce a simple and quick way to tune this reward using the perceptron algorithm.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Murray, Kenton and Chiang, David},
	month = aug,
	year = {2018},
	note = {arXiv:1808.10006 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{yang_breaking_2018,
	title = {Breaking the {Beam} {Search} {Curse}: {A} {Study} of ({Re}-){Scoring} {Methods} and {Stopping} {Criteria} for {Neural} {Machine} {Translation}},
	shorttitle = {Breaking the {Beam} {Search} {Curse}},
	url = {http://arxiv.org/abs/1808.09582},
	doi = {10.48550/arXiv.1808.09582},
	abstract = {Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Yang, Yilin and Huang, Liang and Ma, Mingbo},
	month = oct,
	year = {2018},
	note = {arXiv:1808.09582 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{klein_opennmt_2017,
	title = {{OpenNMT}: {Open}-{Source} {Toolkit} for {Neural} {Machine} {Translation}},
	shorttitle = {{OpenNMT}},
	url = {http://arxiv.org/abs/1701.02810},
	doi = {10.48550/arXiv.1701.02810},
	abstract = {We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M.},
	month = mar,
	year = {2017},
	note = {arXiv:1701.02810 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
}

@misc{paulus_deep_2017,
	title = {A {Deep} {Reinforced} {Model} for {Abstractive} {Summarization}},
	url = {http://arxiv.org/abs/1705.04304},
	doi = {10.48550/arXiv.1705.04304},
	abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit "exposure bias" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
	month = nov,
	year = {2017},
	note = {arXiv:1705.04304 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{shao_generating_2017,
	title = {Generating {High}-{Quality} and {Informative} {Conversation} {Responses} with {Sequence}-to-{Sequence} {Models}},
	url = {http://arxiv.org/abs/1701.03185},
	doi = {10.48550/arXiv.1701.03185},
	abstract = {Sequence-to-sequence models have been applied to the conversation response generation problem where the source sequence is the conversation history and the target sequence is the response. Unlike translation, conversation responding is inherently creative. The generation of long, informative, coherent, and diverse responses remains a hard task. In this work, we focus on the single turn setting. We add self-attention to the decoder to maintain coherence in longer responses, and we propose a practical approach, called the glimpse-model, for scaling to large datasets. We introduce a stochastic beam-search algorithm with segment-by-segment reranking which lets us inject diversity earlier in the generation process. We trained on a combined data set of over 2.3B conversation messages mined from the web. In human evaluation studies, our method produces longer responses overall, with a higher proportion rated as acceptable and excellent as length increases, compared to baseline sequence-to-sequence models with explicit length-promotion. A back-off strategy produces better responses overall, in the full spectrum of lengths.},
	urldate = {2023-12-18},
	publisher = {arXiv},
	author = {Shao, Louis and Gouws, Stephan and Britz, Denny and Goldie, Anna and Strope, Brian and Kurzweil, Ray},
	month = jul,
	year = {2017},
	note = {arXiv:1701.03185 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{manjavacas_generation_2019,
	address = {Tokyo, Japan},
	title = {Generation of {Hip}-{Hop} {Lyrics} with {Hierarchical} {Modeling} and {Conditional} {Templates}},
	url = {https://aclanthology.org/W19-8638},
	doi = {10.18653/v1/W19-8638},
	abstract = {This paper addresses Hip-Hop lyric generation with conditional Neural Language Models. We develop a simple yet effective mechanism to extract and apply conditional templates from text snippets, and show—on the basis of a large-scale crowd-sourced manual evaluation—that these templates significantly improve the quality and realism of the generated snippets. Importantly, the proposed approach enables end-to-end training, targeting formal properties of text such as rhythm and rhyme, which are central characteristics of rap texts. Additionally, we explore how generating text at different scales (e.g. character-level or word-level) affects the quality of the output. We find that a hybrid form—a hierarchical model that aims to integrate Language Modeling at both word and character-level scales—yields significant improvements in text quality, yet surprisingly, cannot exploit conditional templates to their fullest extent. Our findings highlight that text generation models based on Recurrent Neural Networks (RNN) are sensitive to the modeling scale and call for further research on the observed differences in effectiveness of the conditioning mechanism at different scales.},
	urldate = {2023-11-12},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Manjavacas, Enrique and Kestemont, Mike and Karsdorp, Folgert},
	editor = {van Deemter, Kees and Lin, Chenghua and Takamura, Hiroya},
	month = oct,
	year = {2019},
	pages = {301--310},
}

@misc{noauthor_epfl-dlabunfun_nodate,
	title = {epfl-dlab/unfun: {Code} and data for the {AAAI}'19 paper "{Reverse}-{Engineering} {Satire}, or '{Paper} on {Computational} {Humor} {Accepted} {Despite} {Making} {Serious} {Advances}'"},
	shorttitle = {epfl-dlab/unfun},
	url = {https://github.com/epfl-dlab/unfun},
	abstract = {Code and data for the AAAI\&\#39;19 paper \&quot;Reverse-Engineering Satire, or \&\#39;Paper on Computational Humor Accepted Despite Making Serious Advances\&\#39;\&quot; - epfl-dlab/unfun: Code and data f...},
	language = {en},
	urldate = {2023-10-19},
	journal = {GitHub},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {Humicroedit} {Dataset}},
	url = {https://paperswithcode.com/dataset/humicroedit},
	abstract = {Humicroedit is a humorous headline dataset. The data consists of regular English news headlines paired with versions of the same headlines that contain simple replacement edits designed to make them funny. The authors carefully curated crowdsourced editors to create funny headlines and judges to score a to a total of 15,095 edited headlines, with five judges per headline.},
	language = {en},
	urldate = {2023-10-19},
}

@inproceedings{sasaki_pilot_2021,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Pilot {Study}: {Does} {Phonological} {Similarity} of {Words} {Enhance} {Humor} in “{Soramimi}” {Parody} {Songs}?},
	isbn = {978-3-030-78635-9},
	shorttitle = {Pilot {Study}},
	doi = {10.1007/978-3-030-78635-9_77},
	abstract = {Humor is essential to establish more natural and enjoyable human–computer interactions, and researchers have been working on developing a way to automatically generate humor. This study aims to explore a better way to automatically generate Japanese common humor “soramimi”, and understand how the humor occurs. Soramimi is a type of parody song in which the original lyrics are replaced by different words that have similar pronunciations. Although a previous study proposed an algorithm to replace input text with homophonic soramimi text, the mechanism of soramimi humor is still unclear. Based on the incongruity-resolution model, we hypothesized that phonological similarity between the parody and the original lyrics enhances humor in soramimi. A subjective experiment was conducted in which the phonological similarity and humor of fifteen soramimi parody lyrics were evaluated. The results indicated that the phonological similarity of soramimi was positively correlated with its humorousness. Exploring other factors that affect humorousness and the development of an automatic generation system for soramimi lyrics based on the identified factors are topics for our future research.},
	language = {en},
	booktitle = {{HCI} {International} 2021 - {Posters}},
	publisher = {Springer International Publishing},
	author = {Sasaki, Masaru and Shimaya, Jiro and Nakamura, Yutaka},
	editor = {Stephanidis, Constantine and Antona, Margherita and Ntoa, Stavroula},
	year = {2021},
	keywords = {Humor, Incongruity-resolution model, Soramimi},
	pages = {603--608},
}

@article{andersson_ai_nodate,
	title = {{AI} generated parody lyrics},
	abstract = {This essay concerns AI generation of parody lyrics, based on an existing program called Weird AI Yankovic (hereafter WAIY) that is developed by Mark O. Riedl at the Georgia Institute of Technology. It is a Python program that loads the powerful language models GPT-2 and XLNet and takes a short parody context description as well as an existing song text as input. While the syllable and rhyme structure are retained, the words are replaced with newly generated text based on the entered context. The result is new lyrics that can be sung to the original melody but are intended to gain comical qualities given the new content.},
	language = {en},
	author = {Andersson, Daniel},
}

@inproceedings{valitutti_let_2013,
	address = {Sofia, Bulgaria},
	title = {“{Let} {Everything} {Turn} {Well} in {Your} {Wife}”: {Generation} of {Adult} {Humor} {Using} {Lexical} {Constraints}},
	shorttitle = {“{Let} {Everything} {Turn} {Well} in {Your} {Wife}”},
	url = {https://aclanthology.org/P13-2044},
	urldate = {2023-10-17},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Valitutti, Alessandro and Toivonen, Hannu and Doucet, Antoine and Toivanen, Jukka M.},
	month = aug,
	year = {2013},
	pages = {243--248},
}

@misc{noauthor_cmu_nodate,
	title = {The {CMU} {Pronouncing} {Dictionary}},
	url = {http://www.speech.cs.cmu.edu/cgi-bin/cmudict},
	urldate = {2023-10-17},
}

@inproceedings{ozbal_brainsup_2013,
	address = {Sofia, Bulgaria},
	title = {{BRAINSUP}: {Brainstorming} {Support} for {Creative} {Sentence} {Generation}},
	shorttitle = {{BRAINSUP}},
	url = {https://aclanthology.org/P13-1142},
	urldate = {2023-10-17},
	booktitle = {Proceedings of the 51st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Özbal, Gözde and Pighin, Daniele and Strapparava, Carlo},
	month = aug,
	year = {2013},
	pages = {1446--1455},
}

@inproceedings{oliveira_weirdanalogymatic_2020,
	title = {{WeirdAnalogyMatic}: {Experimenting} with {Analogy} for {Lyrics} {Transformation}},
	shorttitle = {{WeirdAnalogyMatic}},
	url = {https://www.semanticscholar.org/paper/WeirdAnalogyMatic%3A-Experimenting-with-Analogy-for-Oliveira/3564ece8bd2b7c57ce005cc91098ebfa766b7bc2},
	abstract = {This paper is on the transformation of text relying mostly on a common analogy vector operation, computed in a distributional model, i.e., static word embeddings. Given a theme, original song lyrics have their words replaced by new ones, related to the new theme as the original words are to the original theme. As this is not enough for producing good lyrics, towards more coherent and singable text, constraints are gradually applied to the replacements. Human opinions confirmed that more balanced lyrics are obtained when there is a one-toone mapping between original words and their replacement; only content words are replaced; and the replacement has the same part-of-speech and rhythm as the original word.},
	urldate = {2023-10-03},
	author = {Oliveira, Hugo Gonçalo},
	year = {2020},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{singh_automated_2018,
	title = {{AUTOMATED} {LYRICAL} {NARRATIVE} {WRITING}},
	url = {https://scholarworks.sjsu.edu/etd_projects/617},
	doi = {https://doi.org/10.31979/etd.756z-tvbf},
	journal = {Master's Projects},
	author = {Singh, Divya},
	month = apr,
	year = {2018},
}

@misc{rusia_xlnet_2019,
	title = {{XLNet} speaks. {Comparison} to {GPT}-2},
	url = {https://amanrusia.medium.com/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e},
	abstract = {This was not me, but the XLNet model talking (prompt text is in the bold). For more samples and quick usage go to…},
	language = {en},
	urldate = {2023-10-16},
	journal = {Medium},
	author = {Rusia, Aman},
	month = jul,
	year = {2019},
}

@article{oliveira_tra--lyrics_2015,
	title = {Tra-la-{Lyrics} 2.0: {Automatic} {Generation} of {Song} {Lyrics} on a {Semantic} {Domain}},
	volume = {6},
	shorttitle = {Tra-la-{Lyrics} 2.0},
	url = {https://sciendo.com/article/10.1515/jagi-2015-0005},
	doi = {10.1515/jagi-2015-0005},
	abstract = {AbstractTra-la-Lyrics is a system that generates song lyrics automatically. In its original version, the main focus was to produce text where stresses matched the rhythm of given melodies. There were no concerns on whether the text made sense or if the selected words shared some kind of semantic association. In this article, we describe the development of a new version of Tra-la-Lyrics, where text is generated on a semantic domain, defined by one or more seed words. This effort involved the integration of the original rhythm module of Tra-la-Lyrics in PoeTryMe, a generic platform that generates poetry with semantically coherent sentences. To measure our progress, the rhythm, the rhymes, and the semantic coherence in lyrics produced by the original Tra-la-Lyrics were analysed and compared with lyrics produced by the new instantiation of this system, dubbed Tra-la-Lyrics 2.0. The analysis showed that, in the lyrics by the new system, words have higher semantic association among them and with the given seeds, while the rhythm is still matched and rhymes are present. The previous analysis was complemented with a crowdsourced evaluation, where contributors answered a survey about relevant features of lyrics produced by the previous and the current versions of Tra-la-Lyrics. Though},
	language = {en},
	number = {1},
	urldate = {2023-10-16},
	journal = {Journal of Artificial General Intelligence},
	author = {Oliveira, Hugo Gonçalo},
	month = dec,
	year = {2015},
	pages = {87--110},
}

@book{goncalo_oliveira_poetryme_2012,
	title = {{PoeTryMe}: a versatile platform for poetry generation},
	volume = {1, article 21},
	shorttitle = {{PoeTryMe}},
	author = {Gonçalo Oliveira, Hugo},
	month = aug,
	year = {2012},
}

@phdthesis{lau_application_2021,
	title = {Application of {Machine} {Learning} {Model} in {Generating} {Song} {Lyrics}},
	url = {https://escholarship.org/uc/item/77d7c3hh},
	abstract = {(Abstract omitted for brevity)},
	language = {en},
	urldate = {2023-10-16},
	school = {UCLA},
	author = {Lau, Wesley},
	year = {2021},
}

@book{moradi_building_2013,
	title = {Building {Humor} {Ontology} {Using} www.uncyclopedia.co},
	author = {Moradi, Mehdi and Jamshidlou, Paria},
	month = jan,
	year = {2013},
}

@article{_lyrics_nodate,
	title = {Lyrics and {Vocal} {Melody} {Generation} conditioned on {Accompaniment}},
	language = {el},
	author = {Μελίστας, Θωμάς},
}

@article{goncalo_oliveira_automatic_2023,
	title = {Automatic generation of creative text in {Portuguese}: an overview},
	issn = {1574-0218},
	shorttitle = {Automatic generation of creative text in {Portuguese}},
	url = {https://doi.org/10.1007/s10579-023-09646-3},
	doi = {10.1007/s10579-023-09646-3},
	abstract = {Creativity is an inherently human skill, and thus one of the goals of Artificial Intelligence. Specifically, linguistic computational creativity deals with the autonomous generation of linguistically-creative artefacts. Here, we present four types of text that can be tackled in this scope—poetry, humour, riddles, and headlines—and overview computational systems developed for their generation in Portuguese. Adopted approaches are described and illustrated with generated examples, and the key role of underlying computational linguistic resources is highlighted. The future of such systems is further discussed together with the exploration of neural approaches for text generation. While overviewing such systems, we hope to disseminate the area among the community of the computational processing of the Portuguese language.},
	language = {en},
	urldate = {2023-10-16},
	journal = {Language Resources and Evaluation},
	author = {Gonçalo Oliveira, Hugo},
	month = may,
	year = {2023},
	keywords = {Computational creativity, Headlines, Humour, Language resources, Poetry, Riddles},
}

@article{oliveira_exploring_nodate,
	title = {Exploring a {Masked} {Language} {Model} for {Creative} {Text} {Transformation}},
	abstract = {We explore a masked-language model based on BERT for shifting the meaning of text towards a target theme. Content words in the original text are masked and the model provides a list of ﬁlling candidates, out of which one is selected based on its similarity to the theme and constraints regarding morphology and metre. Experimentation is performed with Portuguese song lyrics and trade-offs between grammaticality, semantics, form, and novelty are analysed. We conﬁrm that BERT is a useful tool for creative text transformation.},
	language = {en},
	author = {Oliveira, Hugo Goncalo},
}

@misc{elzohbi_creative_2023,
	title = {Creative {Data} {Generation}: {A} {Review} {Focusing} on {Text} and {Poetry}},
	shorttitle = {Creative {Data} {Generation}},
	url = {http://arxiv.org/abs/2305.08493},
	doi = {10.48550/arXiv.2305.08493},
	abstract = {The rapid advancement in machine learning has led to a surge in automatic data generation, making it increasingly challenging to differentiate between naturally or human-generated data and machine-generated data. Despite these advancements, the generation of creative data remains a challenge. This paper aims to investigate and comprehend the essence of creativity, both in general and within the context of natural language generation. We review various approaches to creative writing devices and tasks, with a specific focus on the generation of poetry. We aim to shed light on the challenges and opportunities in the field of creative data generation.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Elzohbi, Mohamad and Zhao, Richard},
	month = jun,
	year = {2023},
	note = {arXiv:2305.08493 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ram_say_2021,
	address = {New York, NY, USA},
	series = {{HAI} '21},
	title = {Say {What}? {Collaborative} {Pop} {Lyric} {Generation} {Using} {Multitask} {Transfer} {Learning}},
	isbn = {978-1-4503-8620-3},
	shorttitle = {Say {What}?},
	url = {https://dl.acm.org/doi/10.1145/3472307.3484175},
	doi = {10.1145/3472307.3484175},
	abstract = {Lyric generation is a popular sub-field of natural language generation that has seen growth in recent years. Pop lyrics are of unique interest due to the genre’s unique style and content, in addition to the high level of collaboration that goes on behind the scenes in the professional pop songwriting process. In this paper, we present a collaborative line-level lyric generation system that utilizes transfer-learning via the T5 transformer model, which, till date, has not been used to generate pop lyrics. By working and communicating directly with professional songwriters, we develop a model that is able to learn lyrical and stylistic tasks like rhyming, matching line beat requirements, and ending lines with specific target words. Our approach compares favorably to existing methods for multiple datasets and yields positive results from our online studies and interviews with industry songwriters.},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Human}-{Agent} {Interaction}},
	publisher = {Association for Computing Machinery},
	author = {Ram, Naveen and Gummadi, Tanay and Bhethanabotla, Rahul and Savery, Richard J and Weinberg, Gil},
	month = nov,
	year = {2021},
	keywords = {collaborative AI, lyric generation, natural language generation, pop music, transfer learning, transformers},
	pages = {165--173},
}

@article{chang_singability-enhanced_2021,
	title = {Singability-enhanced lyric generator with music style transfer},
	volume = {168},
	issn = {0140-3664},
	url = {https://www.sciencedirect.com/science/article/pii/S0140366421000104},
	doi = {10.1016/j.comcom.2021.01.002},
	abstract = {The lyrics generator should consider the context and the singability of the songs because every song expresses a story through the context of lyrics, and the lyrics should sound with the music well. Therefore, this study proposes a framework to generate the singable lyrics, and the context of lyrics should fit the given musical style. For the context, this study adopts the GPT-2 model which is powerful for text generation. The conditional GPT-2 model can be used to generate lyrics according to the given style. For suitable for singing, this study adjusts the structure and rhyme of lyrics through the use of a syntactic parser and a rhyme modification module. With automatic and human evaluations, the experimental results show that the proposed method can generate lyrics with high structural consistency, rhyme consistency, and originality according to the given music style.},
	urldate = {2023-10-16},
	journal = {Computer Communications},
	author = {Chang, Jia-Wei and Hung, Jason C. and Lin, Kuan-Cheng},
	month = feb,
	year = {2021},
	keywords = {GPT-2, Lyric generator, Music style transfer, Natural language processing},
	pages = {33--53},
}

@inproceedings{gatti_automatic_2017,
	address = {Mountain View California USA},
	title = {Automatic {Generation} of {Lyrics} {Parodies}},
	isbn = {978-1-4503-4906-2},
	url = {https://dl.acm.org/doi/10.1145/3123266.3123410},
	doi = {10.1145/3123266.3123410},
	language = {en},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 25th {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Gatti, Lorenzo and Özbal, Gözde and Stock, Oliviero and Strapparava, Carlo},
	month = oct,
	year = {2017},
	pages = {485--491},
}

@inproceedings{nikolov_rapformer_2020,
	title = {Rapformer: {Conditional} {Rap} {Lyrics} {Generation} with {Denoising} {Autoencoders}},
	shorttitle = {Rapformer},
	url = {https://www.semanticscholar.org/paper/Rapformer%3A-Conditional-Rap-Lyrics-Generation-with-Nikolov-Malmi/92c6252075053816eaffc2081d7aac4bb7ad154d},
	abstract = {The ability to combine symbols to generate language is a defining characteristic of human intelligence, particularly in the context of artistic story-telling through lyrics. We develop a method for synthesizing a rap verse based on the content of any text (e.g., a news article), or for augmenting pre-existing rap lyrics. Our method, called Rapformer, is based on training a Transformer-based denoising autoencoder to reconstruct rap lyrics from content words extracted from the lyrics, trying to preserve the essential meaning, while matching the target style. Rapformer features a novel BERT-based paraphrasing scheme for rhyme enhancement which increases the average rhyme density of output lyrics by 10\%. Experimental results on three diverse input domains show that Rapformer is capable of generating technically fluent verses that offer a good trade-off between content preservation and style transfer. Furthermore, a Turing-test-like experiment reveals that Rapformer fools human lyrics experts 25\% of the time.},
	urldate = {2023-10-16},
	author = {Nikolov, Nikola I. and Malmi, Eric and Northcutt, Curtis G. and Parisi, Loreto},
	month = dec,
	year = {2020},
}

@book{vechtomova_generating_2018,
	title = {Generating lyrics with variational autoencoder and multi-modal artist embeddings},
	abstract = {We present a system for generating song lyrics lines conditioned on the style of a specified artist. The system uses a variational autoencoder with artist embeddings. We propose the pre-training of artist embeddings with the representations learned by a CNN classifier, which is trained to predict artists based on MEL spectrograms of their song clips. This work is the first step towards combining audio and text modalities of songs for generating lyrics conditioned on the artist's style. Our preliminary results suggest that there is a benefit in initializing artists' embeddings with the representations learned by a spectrogram classifier.},
	author = {Vechtomova, Olga and Bahuleyan, Hareesh and Ghabussi, Amirpasha and John, Vineet},
	month = dec,
	year = {2018},
}

@inproceedings{xue_deeprapper_2021,
	address = {Online},
	title = {{DeepRapper}: {Neural} {Rap} {Generation} with {Rhyme} and {Rhythm} {Modeling}},
	shorttitle = {{DeepRapper}},
	url = {https://aclanthology.org/2021.acl-long.6},
	doi = {10.18653/v1/2021.acl-long.6},
	abstract = {Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms. Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance. In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms. Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats. Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms. Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling. To our knowledge, DeepRapper is the first system to generate rap with both rhymes and rhythms. Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms.},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Lanqing and Song, Kaitao and Wu, Duocai and Tan, Xu and Zhang, Nevin L. and Qin, Tao and Zhang, Wei-Qiang and Liu, Tie-Yan},
	month = aug,
	year = {2021},
	pages = {69--81},
}

@inproceedings{liu_chipsong_2022,
	address = {Dublin, Ireland},
	title = {{ChipSong}: {A} {Controllable} {Lyric} {Generation} {System} for {Chinese} {Popular} {Song}},
	shorttitle = {{ChipSong}},
	url = {https://aclanthology.org/2022.in2writing-1.13},
	doi = {10.18653/v1/2022.in2writing-1.13},
	abstract = {In this work, we take a further step towards satisfying practical demands in Chinese lyric generation from musical short-video creators, in respect of the challenges on songs' format constraints, creating specific lyrics from open-ended inspiration inputs, and language rhyme grace. One representative detail in these demands is to control lyric format at word level, that is, for Chinese songs, creators even expect fix-length words on certain positions in a lyric to match a special melody, while previous methods lack such ability. Although recent lyric generation community has made gratifying progress, most methods are not comprehensive enough to simultaneously meet these demands. As a result, we propose ChipSong, which is an assisted lyric generation system built based on a Transformer-based autoregressive language model architecture, and generates controlled lyric paragraphs fit for musical short-video display purpose, by designing 1) a novel Begin-Internal-End (BIE) word-granularity embedding sequence with its guided attention mechanism for word-level length format control, and an explicit symbol set for sentence-level length format control; 2) an open-ended trigger word mechanism to guide specific lyric contents generation; 3) a paradigm of reverse order training and shielding decoding for rhyme control. Extensive experiments show that our ChipSong generates fluent lyrics, with assuring the high consistency to pre-determined control conditions.},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the {First} {Workshop} on {Intelligent} and {Interactive} {Writing} {Assistants} ({In2Writing} 2022)},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Nayu and Han, Wenjing and Liu, Guangcan and Peng, Da and Zhang, Ran and Wang, Xiaorui and Ruan, Huabin},
	month = may,
	year = {2022},
	pages = {85--95},
}

@article{yan_i_nodate,
	title = {i, {Poet}: {Automatic} {Poetry} {Composition} through {Recurrent} {Neural} {Networks} with {Iterative} {Polishing} {Schema}},
	abstract = {Part of the long lasting cultural heritage of humanity is the art of classical poems, which are created by ﬁtting words into certain formats and representations. Automatic poetry composition by computers is considered as a challenging problem which requires high Artiﬁcial Intelligence assistance. This study attracts more and more attention in the research community. In this paper, we formulate the poetry composition task as a natural language generation problem using recurrent neural networks. Given user speciﬁed writing intents, the system generates a poem via sequential language modeling. Unlike the traditional one-pass generation for previous neural network models, poetry composition needs polishing to satisfy certain requirements. Hence, we propose a new generative model with a polishing schema, and output a reﬁned poem composition. In this way, the poem is generated incrementally and iteratively by reﬁning each line. We run experiments based on large datasets of 61,960 classic poems in Chinese. A comprehensive evaluation, using perplexity and BLEU measurements as well as human judgments, has demonstrated the effectiveness of our proposed approach.},
	language = {en},
	author = {Yan, Rui},
}

@inproceedings{stegeren_churnalist_2019,
	title = {Churnalist: {Fictional} {Headline} {Generation} for {Context}-appropriate {Flavor} {Text}},
	shorttitle = {Churnalist},
	url = {https://research.utwente.nl/en/publications/churnalist-fictional-headline-generation-for-context-appropriate-},
	language = {English},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Computational} {Creativity}},
	publisher = {Association for Computational Creativity},
	author = {Stegeren, Judith van and Theune, Mariet},
	month = jun,
	year = {2019},
	pages = {65--72},
}

@book{toivanen_corpus-based_2012,
	title = {Corpus-based generation of content and form in poetry},
	abstract = {We employ a corpus-based approach to generate content and form in poetry. The main idea 
is to use two different corpora, on one hand, to provide semantic content for new poems, and
on the other hand, to generate a specific grammatical and poetic structure. The approach
uses text mining methods, morphological analysis, and morphological synthesis to produce
poetry in Finnish. We present some promising results obtained via the combination of these
methods and preliminary evaluation results of poetry generated by the system.},
	author = {Toivanen, Jukka and Toivonen, Hannu and Valitutti, Alessandro and Gross, Oskar},
	month = may,
	year = {2012},
}

@book{repar_bislon_2018,
	title = {{BISLON}: {BISociative} {SLOgaN} generation based on stylistic literary devices},
	shorttitle = {{BISLON}},
	abstract = {We describe a novel slogan generator that employs bisociation in combination with the selection of stylistic literary devices. Advertising slogans are a key marketing tool for every company and a memorable slogan pro- vides an advantage on the market. A good slogan is catchy and unique and projects the values of the company. To get an insight in construction of such slogans, we first analyze a large corpus of advertising slogans in terms of alliteration, assonance, consonance and rhyme. Then we develop an approach for constructing slogans that contain these stylistic devices which can help make the slogans easy to remember. At the same time, we use bisociation to imprint a unique message into the slogan by allowing the user to specify the original and bisociated domains from where the generator selects the words. These word sets are first expanded with the help of FastText embeddings and then used to fill in the empty slots in slogan skeletons generated from a database of existing slogans. We use a language model to increase semantical cohesion of generated slogans and a relevance evaluation system to score the slogans by their connectedness to the selected domains. The evaluation of generated slogans for two companies shows that even if slogan generation is a hard problem, we can find some generated slogans that are suit- able for the use in production without any modification and a much larger number of slogans that are positively evaluated according to at least one criteria (e.g., humor, catchiness).},
	author = {Repar, Andraž and Martinc, Matej and Znidaršič, Martiň and Pollak, Senja},
	month = jun,
	year = {2018},
}

@inproceedings{potash_ghostwriter_2015,
	address = {Lisbon, Portugal},
	title = {{GhostWriter}: {Using} an {LSTM} for {Automatic} {Rap} {Lyric} {Generation}},
	shorttitle = {{GhostWriter}},
	url = {https://aclanthology.org/D15-1221},
	doi = {10.18653/v1/D15-1221},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Potash, Peter and Romanov, Alexey and Rumshisky, Anna},
	month = sep,
	year = {2015},
	pages = {1919--1924},
}

@inproceedings{manning_stanford_2014,
	address = {Baltimore, Maryland},
	title = {The {Stanford} {CoreNLP} {Natural} {Language} {Processing} {Toolkit}},
	url = {https://aclanthology.org/P14-5010},
	doi = {10.3115/v1/P14-5010},
	urldate = {2023-10-14},
	booktitle = {Proceedings of 52nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Manning, Christopher and Surdeanu, Mihai and Bauer, John and Finkel, Jenny and Bethard, Steven and McClosky, David},
	month = jun,
	year = {2014},
	pages = {55--60},
}

@inproceedings{hamalainen_lets_2019,
	address = {Tokyo, Japan},
	title = {Let's {FACE} it. {Finnish} {Poetry} {Generation} with {Aesthetics} and {Framing}},
	url = {https://aclanthology.org/W19-8637},
	doi = {10.18653/v1/W19-8637},
	abstract = {We present a creative poem generator for the morphologically rich Finnish language. Our method falls into the master-apprentice paradigm, where a computationally creative genetic algorithm teaches a BRNN model to generate poetry. We model several parts of poetic aesthetics in the fitness function of the genetic algorithm, such as sonic features, semantic coherence, imagery and metaphor. Furthermore, we justify the creativity of our method based on the FACE theory on computational creativity and take additional care in evaluating our system by automatic metrics for concepts together with human evaluation for aesthetics, framing and expressions.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Hämäläinen, Mika and Alnajjar, Khalid},
	month = oct,
	year = {2019},
	pages = {290--300},
}

@inproceedings{hamalainen_generating_2019,
	address = {Hong Kong, China},
	title = {Generating {Modern} {Poetry} {Automatically} in {Finnish}},
	url = {https://aclanthology.org/D19-1617},
	doi = {10.18653/v1/D19-1617},
	abstract = {We present a novel approach for generating poetry automatically for the morphologically rich Finnish language by using a genetic algorithm. The approach improves the state of the art of the previous Finnish poem generators by introducing a higher degree of freedom in terms of structural creativity. Our approach is evaluated and described within the paradigm of computational creativity, where the fitness functions of the genetic algorithm are assimilated with the notion of aesthetics. The output is considered to be a poem 81.5\% of the time by human evaluators.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Hämäläinen, Mika and Alnajjar, Khalid},
	month = nov,
	year = {2019},
	pages = {5999--6004},
}

@inproceedings{goncalo_oliveira_survey_2017,
	address = {Santiago de Compostela, Spain},
	title = {A {Survey} on {Intelligent} {Poetry} {Generation}: {Languages}, {Features}, {Techniques}, {Reutilisation} and {Evaluation}},
	shorttitle = {A {Survey} on {Intelligent} {Poetry} {Generation}},
	url = {https://aclanthology.org/W17-3502},
	doi = {10.18653/v1/W17-3502},
	abstract = {Poetry generation is becoming popular among researchers of Natural Language Generation, Computational Creativity and, broadly, Artificial Intelligence. To produce text that may be regarded as poetry, poetry generation systems are typically knowledge-intensive and have to deal with several levels of language, from lexical to semantics. Interest on the topic resulted in the development of several poetry generators described in the literature, with different features covered or handled differently, by a broad range of alternative approaches, as well as different perspectives on evaluation, another challenging aspect due the underlying subjectivity. This paper surveys intelligent poetry generators around a set of relevant axis for poetry generation – targeted languages, form and content features, techniques, reutilisation of material, and evaluation – and aims to organise work developed on this topic so far.},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Gonçalo Oliveira, Hugo},
	month = sep,
	year = {2017},
	pages = {11--20},
}

@book{goncalo_oliveira_poetry_2016,
	title = {Poetry from {Concept} {Maps} – {Yet} {Another} {Adaptation} of {PoeTryMe}'s {Flexible} {Architecture}},
	abstract = {This paper describes a preliminary effort on adapting an existing poetry generation platform, PoeTryMe, to produce poetry from concept maps, extracted from textual documents. Instead of a set of given words that would constrain a general language semantic network, the presented adaptation dynamically sets the semantic network to the given concept map. As the relations in the concept maps are open, a new generation grammar had also to be created. Besides an architectural overview of the system, this paper is illustrated with several generated poems, together with the maps that originated them. Still, although poetic features are present and the content of the maps is reflected, they do not transmit exactly the meaning of the original document, due both to limitations on the grammars and issues on the quality of the maps.},
	author = {Gonçalo Oliveira, Hugo and Alves, Ana},
	month = jun,
	year = {2016},
}

@misc{noauthor_wordnet_nodate,
	title = {{WordNet}: {An} {Electronic} {Lexical} {Database} {\textbar} {MIT} {Press} {eBooks} {\textbar} {IEEE} {Xplore}},
	url = {https://ieeexplore.ieee.org/book/6267389},
	urldate = {2023-10-14},
}

@inproceedings{bay_text_2017,
	title = {Text {Transformation} {Via} {Constraints} and {Word} {Embedding}},
	url = {https://www.semanticscholar.org/paper/Text-Transformation-Via-Constraints-and-Word-Bay-Bodily/ff2975cf624280e24b874c9efbb45baf5398a143},
	abstract = {In order to provide resources for artistic communities and further the linguistic capabilities of computationally creative systems, we present a computational process for creative text transformation and evaluation. Its purpose is to help solve the fundamental problem posed by the ﬁeld of natural language generation, which is to computationally generate human-readable language. Our process entails the use of 1) vector word embedding to approximate meaning and 2) constraints to guide word replacement. We introduce intentions as objects that drive the generation of creative artefacts; a target theme, emotion, meter, or rhyme scheme may be represented via intention. Our implementation of this process, Lyrist , is oriented around poetry and song lyrics and successfully produces syntactically correct, human-voiced text. A preliminary evaluation suggests that our process successfully evokes human-recognizable sentiments and that even familiar texts are difﬁcult to recognize after undergoing transformation.},
	urldate = {2023-10-14},
	author = {Bay, Benjamin and Bodily, P. and Ventura, D.},
	year = {2017},
}

@inproceedings{agirrezabal_pos-tag_2013,
	address = {Sofia, Bulgaria},
	title = {{POS}-{Tag} {Based} {Poetry} {Generation} with {WordNet}},
	url = {https://aclanthology.org/W13-2121},
	urldate = {2023-10-14},
	booktitle = {Proceedings of the 14th {European} {Workshop} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Agirrezabal, Manex and Arrieta, Bertol and Astigarraga, Aitzol and Hulden, Mans},
	month = aug,
	year = {2013},
	pages = {162--166},
}

@misc{xiao_efficient_2023,
	title = {Efficient {Streaming} {Language} {Models} with {Attention} {Sinks}},
	url = {http://arxiv.org/abs/2309.17453},
	doi = {10.48550/arXiv.2309.17453},
	abstract = {Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink'' even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.},
	urldate = {2023-10-09},
	publisher = {arXiv},
	author = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	month = sep,
	year = {2023},
	note = {arXiv:2309.17453 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{loshchilov_decoupled_2017,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://arxiv.org/abs/1711.05101v3},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = nov,
	year = {2017},
}

@inproceedings{dolan_automatically_2005,
	title = {Automatically {Constructing} a {Corpus} of {Sentential} {Paraphrases}},
	url = {https://aclanthology.org/I05-5002},
	urldate = {2023-10-09},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Paraphrasing} ({IWP2005})},
	author = {Dolan, William B. and Brockett, Chris},
	year = {2005},
}

@inproceedings{van_de_cruys_automatic_2020,
	address = {Online},
	title = {Automatic {Poetry} {Generation} from {Prosaic} {Text}},
	url = {https://aclanthology.org/2020.acl-main.223},
	doi = {10.18653/v1/2020.acl-main.223},
	abstract = {In the last few years, a number of successful approaches have emerged that are able to adequately model various aspects of natural language. In particular, language models based on neural networks have improved the state of the art with regard to predictive language modeling, while topic models are successful at capturing clear-cut, semantic dimensions. In this paper, we will explore how these approaches can be adapted and combined to model the linguistic and literary aspects needed for poetry generation. The system is exclusively trained on standard, non-poetic text, and its output is constrained in order to confer a poetic character to the generated verse. The framework is applied to the generation of poems in both English and French, and is equally evaluated for both languages. Even though it only uses standard, non-poetic text as input, the system yields state of the art results for poetry generation.},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Van de Cruys, Tim},
	month = jul,
	year = {2020},
	pages = {2471--2480},
}

@inproceedings{yi_automatic_2018,
	address = {Brussels, Belgium},
	title = {Automatic {Poetry} {Generation} with {Mutual} {Reinforcement} {Learning}},
	url = {https://aclanthology.org/D18-1353},
	doi = {10.18653/v1/D18-1353},
	abstract = {Poetry is one of the most beautiful forms of human language art. As a crucial step towards computer creativity, automatic poetry generation has drawn researchers' attention for decades. In recent years, some neural models have made remarkable progress in this task. However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in loss-evaluation mismatch. Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood. To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema. We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance. We experiment on Chinese poetry. Based on a strong basic model, our method achieves better results and outperforms the current state-of-the-art method.},
	urldate = {2023-10-06},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yi, Xiaoyuan and Sun, Maosong and Li, Ruoyu and Li, Wenhao},
	month = oct,
	year = {2018},
	pages = {3143--3153},
}

@misc{black_gpt-neox-20b_2022,
	title = {{GPT}-{NeoX}-{20B}: {An} {Open}-{Source} {Autoregressive} {Language} {Model}},
	shorttitle = {{GPT}-{NeoX}-{20B}},
	url = {http://arxiv.org/abs/2204.06745},
	doi = {10.48550/arXiv.2204.06745},
	abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe {\textbackslash}model\{\}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06745 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{greene_automatic_2010,
	address = {Cambridge, MA},
	title = {Automatic {Analysis} of {Rhythmic} {Poetry} with {Applications} to {Generation} and {Translation}},
	url = {https://aclanthology.org/D10-1051},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Greene, Erica and Bodrumlu, Tugba and Knight, Kevin},
	month = oct,
	year = {2010},
	pages = {524--533},
}

@misc{yang_generating_2020,
	title = {Generating {Thematic} {Chinese} {Poetry} using {Conditional} {Variational} {Autoencoders} with {Hybrid} {Decoders}},
	url = {http://arxiv.org/abs/1711.07632},
	doi = {10.48550/arXiv.1711.07632},
	abstract = {Computer poetry generation is our first step towards computer writing. Writing must have a theme. The current approaches of using sequence-to-sequence models with attention often produce non-thematic poems. We present a novel conditional variational autoencoder with a hybrid decoder adding the deconvolutional neural networks to the general recurrent neural networks to fully learn topic information via latent variables. This approach significantly improves the relevance of the generated poems by representing each line of the poem not only in a context-sensitive manner but also in a holistic way that is highly related to the given keyword and the learned topic. A proposed augmented word2vec model further improves the rhythm and symmetry. Tests show that the generated poems by our approach are mostly satisfying with regulated rules and consistent themes, and 73.42\% of them receive an Overall score no less than 3 (the highest score is 5).},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Yang, Xiaopeng and Lin, Xiaowen and Suo, Shunda and Li, Ming},
	month = mar,
	year = {2020},
	note = {arXiv:1711.07632 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{holtman_constraint-based_1994,
	title = {A {Constraint}-based {Approach} to {Rhyme}},
	volume = {11},
	issn = {0929-7332, 1569-9919},
	url = {http://www.jbe-platform.com/content/journals/10.1075/avt.11.07hol},
	doi = {10.1075/avt.11.07hol},
	abstract = {In this paper I shall be concerned with the phonological structure of poetic rhyme. I shall try to show that a suitable framework for the analysis of rhyme may be found in the recently developed principles of Optimality Theory as initiated by Prince and Smolensky (1993). My point of departure is the fact that rhyme shows some striking resemblances to the phenomenon of reduplication. Reduplication has been thoroughly studied in the past decade and it has also been the focus of attention of McCarthy and Prince in their recent manuscript Prosodic Morphology I.},
	language = {en},
	urldate = {2023-10-03},
	journal = {Linguistics in the Netherlands},
	author = {Holtman, Astrid},
	month = oct,
	year = {1994},
	pages = {49--60},
}

@incollection{zugarini_neural_2019,
	title = {Neural {Poetry}: {Learning} to {Generate} {Poems} using {Syllables}},
	volume = {11730},
	shorttitle = {Neural {Poetry}},
	url = {http://arxiv.org/abs/1908.08861},
	abstract = {Motivated by the recent progresses on machine learning-based models that learn artistic styles, in this paper we focus on the problem of poem generation. This is a challenging task in which the machine has to capture the linguistic features that strongly characterize a certain poet, as well as the semantics of the poet's production, that are influenced by his personal experiences and by his literary background. Since poetry is constructed using syllables, that regulate the form and structure of poems, we propose a syllable-based neural language model, and we describe a poem generation mechanism that is designed around the poet style, automatically selecting the most representative generations. The poetic work of a target author is usually not enough to successfully train modern deep neural networks, so we propose a multi-stage procedure that exploits non-poetic works of the same author, and also other publicly available huge corpora to learn syntax and grammar of the target language. We focus on the Italian poet Dante Alighieri, widely famous for his Divine Comedy. A quantitative and qualitative experimental analysis of the generated tercets is reported, where we included expert judges with strong background in humanistic studies. The generated tercets are frequently considered to be real by a generic population of judges, with relative difference of 56.25{\textbackslash}\% with respect to the ones really authored by Dante, and expert judges perceived Dante's style and rhymes in the generated text.},
	urldate = {2023-10-03},
	author = {Zugarini, Andrea and Melacci, Stefano and Maggini, Marco},
	year = {2019},
	doi = {10.1007/978-3-030-30490-4_26},
	note = {arXiv:1908.08861 [cs]},
	keywords = {Computer Science - Computation and Language},
	pages = {313--325},
}

@article{toivanen_harnessing_nodate,
	title = {Harnessing {Constraint} {Programming} for {Poetry} {Composition}},
	abstract = {Constraints are a major factor shaping the conceptual space of many areas of creativity. We propose to use constraint programming techniques and off-the-shelf constraint solvers in the creative task of poetry writing. We show how many aspects essential in different poetical forms, and partially even in the level of language syntax and semantics can be represented as interacting constraints.},
	language = {en},
	author = {Toivanen, Jukka M and Jarvisalo, Matti and Toivonen, Hannu},
}

@inproceedings{ko_assessing_2020,
	address = {Dublin, Ireland},
	title = {Assessing {Discourse} {Relations} in {Language} {Generation} from {GPT}-2},
	url = {https://aclanthology.org/2020.inlg-1.8},
	abstract = {Recent advances in NLP have been attributed to the emergence of large-scale pre-trained language models. GPT-2, in particular, is suited for generation tasks given its left-to-right language modeling objective, yet the linguistic quality of its generated text has largely remain unexplored. Our work takes a step in understanding GPT-2's outputs in terms of discourse coherence. We perform a comprehensive study on the validity of explicit discourse relations in GPT-2's outputs under both organic generation and fine-tuned scenarios. Results show GPT-2 does not always generate text containing valid discourse relations; nevertheless, its text is more aligned with human expectation in the fine-tuned scenario. We propose a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Ko, Wei-Jen and Li, Junyi Jessy},
	month = dec,
	year = {2020},
	pages = {52--59},
}

@inproceedings{zhang_automatic_2020,
	title = {Automatic {Generation} {Method} of {Ancient} {Poetry} {Based} on {LSTM}},
	url = {https://ieeexplore.ieee.org/document/9248260},
	doi = {10.1109/ICIEA48937.2020.9248260},
	abstract = {This paper mainly focuses on the literary genre of ancient poetry with a certain rhythm and cadence, and proposes a novel automatic generation model of ancient poetry. The model uses about 300,000 Tang poems and Song poems as training data, uses One-hot encoding to process data, and uses long short-term memory networks (LSTMs) to learn the semantics of ancient poetry texts and conduct research across a single RNN structure. According to the user's requirements, the model can automatically calculate the most relevant coherent words in the context of the selected context, and generate common ancient poetry and specified words. The method of BLEU automatic evaluation and manual evaluation finally demonstrates the effectiveness of the experiment.},
	urldate = {2023-10-03},
	booktitle = {2020 15th {IEEE} {Conference} on {Industrial} {Electronics} and {Applications} ({ICIEA})},
	author = {Zhang, Hanshuang and Zhang, Zhi},
	month = nov,
	year = {2020},
	note = {ISSN: 2158-2297},
	pages = {95--99},
}

@misc{hu_toward_2018,
	title = {Toward {Controlled} {Generation} of {Text}},
	url = {http://arxiv.org/abs/1703.00955},
	doi = {10.48550/arXiv.1703.00955},
	abstract = {Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = sep,
	year = {2018},
	note = {arXiv:1703.00955 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{popescu-belis_constrained_2022,
	address = {Marseille, France},
	title = {Constrained {Language} {Models} for {Interactive} {Poem} {Generation}},
	url = {https://aclanthology.org/2022.lrec-1.377},
	abstract = {This paper describes a system for interactive poem generation, which combines neural language models (LMs) for poem generation with explicit constraints that can be set by users on form, topic, emotion, and rhyming scheme. LMs cannot learn such constraints from the data, which is scarce with respect to their needs even for a well-resourced language such as French. We propose a method to generate verses and stanzas by combining LMs with rule-based algorithms, and compare several approaches for adjusting the words of a poem to a desired combination of topics or emotions. An approach to automatic rhyme setting using a phonetic dictionary is proposed as well. Our system has been demonstrated at public events, and log analysis shows that users found it engaging.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Popescu-Belis, Andrei and Atrio, Àlex and Minder, Valentin and Xanthos, Aris and Luthier, Gabriel and Mattei, Simon and Rodriguez, Antonio},
	month = jun,
	year = {2022},
	pages = {3519--3529},
}

@inproceedings{genzel_poetic_2010,
	address = {Cambridge, MA},
	title = {“{Poetic}” {Statistical} {Machine} {Translation}: {Rhyme} and {Meter}},
	shorttitle = {“{Poetic}” {Statistical} {Machine} {Translation}},
	url = {https://aclanthology.org/D10-1016},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 2010 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Genzel, Dmitriy and Uszkoreit, Jakob and Och, Franz},
	month = oct,
	year = {2010},
	pages = {158--166},
}

@article{zhang_deeprhymes_nodate,
	title = {{DeepRhymes}: {Efficient} {End}-to-end {Conditional} {Rap} {Lyrics} {Generation}},
	abstract = {Computational musical creativity has always been a challenging subject for natural language processing models. To explore the intersection of NLP and music, we examine the generation of rap lyrics while considering human inputs during this creative process. Previous approaches to rap lyric generation were either not conditioned on human input or separated the production of lyrics and injection of rhymes into two steps. We want to improve on current lyric generation literature by creating an end-to-end conditioned system that automatically generates both the lyrics and the rhyme scheme concurrently. To achieve this, we fine-tuned a GPT-2 model on rap and poem verses and another modified GPT-2 model that contains a single layer which processes syllabic information. With a multiplicative attention layer, we combined the outputs of both the modified GPT-2 model and the original GPT-2 model to generate the rap verses. We determined that fine-tuning on the last two layers of the original GTP-2 model and on all rap and poem verses resulted in the best performing model.},
	language = {en},
	author = {Zhang, Bessie and Kung, Catherine and Villa-Renteria, Ivan},
}

@article{rodrigues_lyrics_2022,
	title = {Lyrics {Generation} supported by {Pre}-trained {Models}},
	volume = {35},
	issn = {2334-0762},
	url = {https://journals.flvc.org/FLAIRS/article/view/130607},
	doi = {10.32473/flairs.v35i.130607},
	abstract = {Advancements in neural network architectures have improved the quality of several tasks in computational linguistics. Among the tasks benefited we can mention question and answer systems, dialogue systems, opinion mining and the automatic generation of texts, just to mention a few. Despite the advances, there is still room for contributions, since there are still open problems. In the case of text generation, especially in the musical genre, there are challenges for the production of texts that involve poetry and idioms. In particular, some of these challenges are linked to the treatment of metaphors and metonymy and the generation of paraphrases. This paper presents an analysis of the generation of excerpts of lyrics based on a pre-trained GPT-2 neural network model, after fine-tuning with two lyrics corpora, one in English and one in Portuguese. An analysis of the spelling, syntax and semantics of the generated texts are presented, as well as the discussion about the attempt to find a pattern in the sections generated by the implemented tool. Research demonstrates the potential for using such models in the generation of poetic texts.},
	urldate = {2023-10-03},
	journal = {The International FLAIRS Conference Proceedings},
	author = {Rodrigues, Matheus Augusto and Oliveira, Alcione and Moreira, Alexandra and Possi, Maurilio},
	month = may,
	year = {2022},
}

@article{manurung_using_2012,
	title = {Using genetic algorithms to create meaningful poetic text},
	volume = {24},
	issn = {0952-813X, 1362-3079},
	url = {http://www.tandfonline.com/doi/abs/10.1080/0952813X.2010.539029},
	doi = {10.1080/0952813X.2010.539029},
	abstract = {This article presents a series of experiments in automatically generating poetic texts. We confined our attention to the generation of texts which are syntactically well-formed, meet certain pre-specified patterns of metre and broadly convey some given meaning. Such aspects can be formally defined, thus avoiding the complications of imagery and interpretation that are central to assessing more free forms of verse. Our implemented system, McGONAGALL, applies the genetic algorithm to construct such texts. It uses a sophisticated linguistic formalism to represent its genomic information, from which can be computed the phenotypic information of both semantic representations and patterns of stress. The conducted experiments broadly indicated that relatively meaningful text could be produced if the constraints on metre were relaxed, and precise metric text was possible with loose semantic constraints, but it was difficult to produce text which was both semantically coherent and of high quality metrically.},
	language = {en},
	number = {1},
	urldate = {2023-10-03},
	journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Manurung, Ruli and Ritchie, Graeme and Thompson, Henry},
	month = mar,
	year = {2012},
	pages = {43--64},
}

@article{lewis_syllable_nodate,
	title = {Syllable {Neural} {Language} {Models} for {English} {Poem} {Generation}},
	abstract = {Automatic Poem Generation is an ambitious Natural Language Generation (NLG) problem. Models have to replicate a poem’s structure, rhyme and meter, while producing creative and emotional verses. The lack of abundant poetic corpora, especially for archaic poetry, is a serious limitation for the development of strong poem generators. In this paper, we propose a syllable neural language model for the English language, focusing on the generation of verses with the style of a target author: William Wordsworth. To alleviate the problem of limited available data, we exploit transfer learning. Furthermore, we bias the generation of verses according to a combination of different scoring functions based on meter, style and grammar in order to select lines more compliant with the author’s characteristics. The results of both quantitative and human evaluations show the effectiveness of our approach. In particular, human judges struggle to recognize real verses from the generated ones.},
	language = {en},
	author = {Lewis, Danielle and Zugarini, Andrea and Alonso, Eduardo},
}

@misc{sawicki_bits_2023,
	title = {Bits of {Grass}: {Does} {GPT} already know how to write like {Whitman}?},
	shorttitle = {Bits of {Grass}},
	url = {http://arxiv.org/abs/2305.11064},
	doi = {10.48550/arXiv.2305.11064},
	abstract = {This study examines the ability of GPT-3.5, GPT-3.5-turbo (ChatGPT) and GPT-4 models to generate poems in the style of specific authors using zero-shot and many-shot prompts (which use the maximum context length of 8192 tokens). We assess the performance of models that are not fine-tuned for generating poetry in the style of specific authors, via automated evaluation. Our findings indicate that without fine-tuning, even when provided with the maximum number of 17 poem examples (8192 tokens) in the prompt, these models do not generate poetry in the desired style.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Sawicki, Piotr and Grzes, Marek and Goes, Fabricio and Brown, Dan and Peeperkorn, Max and Khatun, Aisha},
	month = may,
	year = {2023},
	note = {arXiv:2305.11064 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{ormazabal_poelm_2022,
	title = {{PoeLM}: {A} {Meter}- and {Rhyme}-{Controllable} {Language} {Model} for {Unsupervised} {Poetry} {Generation}},
	shorttitle = {{PoeLM}},
	url = {http://arxiv.org/abs/2205.12206},
	doi = {10.48550/arXiv.2205.12206},
	abstract = {Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems following any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Ormazabal, Aitor and Artetxe, Mikel and Agirrezabal, Manex and Soroa, Aitor and Agirre, Eneko},
	month = oct,
	year = {2022},
	note = {arXiv:2205.12206 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{wockener_end--end_2021,
	address = {Punta Cana, Dominican Republic (online)},
	title = {End-to-end style-conditioned poetry generation: {What} does it take to learn from examples alone?},
	shorttitle = {End-to-end style-conditioned poetry generation},
	url = {https://aclanthology.org/2021.latechclfl-1.7},
	doi = {10.18653/v1/2021.latechclfl-1.7},
	abstract = {In this work, we design an end-to-end model for poetry generation based on conditioned recurrent neural network (RNN) language models whose goal is to learn stylistic features (poem length, sentiment, alliteration, and rhyming) from examples alone. We show this model successfully learns the `meaning' of length and sentiment, as we can control it to generate longer or shorter as well as more positive or more negative poems. However, the model does not grasp sound phenomena like alliteration and rhyming, but instead exploits low-level statistical cues. Possible reasons include the size of the training data, the relatively low frequency and difficulty of these sublexical phenomena as well as model biases. We show that more recent GPT-2 models also have problems learning sublexical phenomena such as rhyming from examples alone.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 5th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Wöckener, Jörg and Haider, Thomas and Miller, Tristan and Nguyen, The-Khang and Nguyen, Thanh Tung Linh and Pham, Minh Vu and Belouadi, Jonas and Eger, Steffen},
	month = nov,
	year = {2021},
	pages = {57--66},
}

@misc{nguyen_sp-gpt2_2021,
	title = {{SP}-{GPT2}: {Semantics} {Improvement} in {Vietnamese} {Poetry} {Generation}},
	shorttitle = {{SP}-{GPT2}},
	url = {http://arxiv.org/abs/2110.15723},
	doi = {10.48550/arXiv.2110.15723},
	abstract = {Automatic text generation has garnered growing attention in recent years as an essential step towards computer creativity. Generative Pretraining Transformer 2 (GPT2) is one of the state of the art approaches that have excellent successes. In this paper, we took the first step to investigate the power of GPT2 in traditional Vietnamese poetry generation. In the earlier time, our experiment with base GPT2 was quite good at generating the poem in the proper template. Though it can learn the patterns, including rhyme and tone rules, from the training data, like almost all other text generation approaches, the poems generated still has a topic drift and semantic inconsistency. To improve the cohesion within the poems, we proposed a new model SP-GPT2 (semantic poem GPT2) which was built on the top GPT2 model and an additional loss to constrain context throughout the entire poem. For better evaluation, we examined the methods by both automatic quantitative evaluation and human evaluation. Both automatic and human evaluation demonstrated that our approach can generate poems that have better cohesion without losing the quality due to additional loss. At the same time, we are the pioneers of this topic. We released the first computational scoring module for poems generated in the template containing the style rule dictionary. Additionally, we are the first to publish a Luc-Bat dataset, including 87609 Luc Bat poems, which is equivalent to about 2.6 million sentences, combined with about 83579 poems in other styles was also published for further exploration. The code is available at https://github.com/fsoft-ailab/Poem-Generator},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Nguyen, Tuan and Pham, Hanh and Bui, Truong and Nguyen, Tan and Luong, Duc and Nguyen, Phong},
	month = oct,
	year = {2021},
	note = {arXiv:2110.15723 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{popescu-belis_gpoet_2023,
	address = {Dubrovnik, Croatia},
	title = {{GPoeT}: a {Language} {Model} {Trained} for {Rhyme} {Generation} on {Synthetic} {Data}},
	shorttitle = {{GPoeT}},
	url = {https://aclanthology.org/2023.latechclfl-1.2},
	doi = {10.18653/v1/2023.latechclfl-1.2},
	abstract = {Poem generation with language models requires the modeling of rhyming patterns. We propose a novel solution for learning to rhyme, based on synthetic data generated with a rule-based rhyming algorithm. The algorithm and an evaluation metric use a phonetic dictionary and the definitions of perfect and assonant rhymes. We fine-tune a GPT-2 English model with 124M parameters on 142 MB of natural poems and find that this model generates consecutive rhymes infrequently (11\%). We then fine-tune the model on 6 MB of synthetic quatrains with consecutive rhymes (AABB) and obtain nearly 60\% of rhyming lines in samples generated by the model. Alternating rhymes (ABAB) are more difficult to model because of longer-range dependencies, but they are still learnable from synthetic data, reaching 45\% of rhyming lines in generated samples.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 7th {Joint} {SIGHUM} {Workshop} on {Computational} {Linguistics} for {Cultural} {Heritage}, {Social} {Sciences}, {Humanities} and {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Popescu-Belis, Andrei and Atrio, Àlex R. and Bernath, Bastien and Boisson, Etienne and Ferrari, Teo and Theimer-lienhard, Xavier and Vernikos, Giorgos},
	month = may,
	year = {2023},
	pages = {10--20},
}

@inproceedings{el_bolock_towards_2015,
	address = {New York, NY, USA},
	series = {{SAC} '15},
	title = {Towards automatic poetry generation using constraint handling rules},
	isbn = {978-1-4503-3196-8},
	url = {https://dl.acm.org/doi/10.1145/2695664.2695742},
	doi = {10.1145/2695664.2695742},
	abstract = {In this paper, we discuss the incremental implementation of an autonomous system, capable of generating unique yet meaningful poetry, using Constraint Handling Rules. The system consists of a reasoner, which is responsible for ensuring the grammaticality, poeticness and meaningfulness of the resulting poems, without depending on any external corpora. This is achieved, by incrementally pruning a customized lexicon, to satisfy the constraints enforced by the three modules, until an output poem is produced.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 30th {Annual} {ACM} {Symposium} on {Applied} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {el Bolock, Alia and Abdennadher, Slim},
	month = apr,
	year = {2015},
	keywords = {computational creativity, computational linguistics, constraint handling rules, constraint programming, natural language generation, poetry},
	pages = {1868--1873},
}

@inproceedings{hopkins_automatically_2017,
	address = {Vancouver, Canada},
	title = {Automatically {Generating} {Rhythmic} {Verse} with {Neural} {Networks}},
	url = {https://aclanthology.org/P17-1016},
	doi = {10.18653/v1/P17-1016},
	abstract = {We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54\% of the time. In addition, participants rated a machine-generated poem to be the best amongst all evaluated.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hopkins, Jack and Kiela, Douwe},
	month = jul,
	year = {2017},
	pages = {168--178},
}

@article{ma_switch-gpt_2022,
	title = {Switch-{GPT}: {An} {Effective} {Method} for {Constrained} {Text} {Generation} under {Few}-{Shot} {Settings} ({Student} {Abstract})},
	volume = {36},
	copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Switch-{GPT}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/21642},
	doi = {10.1609/aaai.v36i11.21642},
	abstract = {In real-world applications of natural language generation, target sentences are often required to satisfy some lexical constraints. However, the success of most neural-based models relies heavily on data, which is infeasible for data-scarce new domains. In this work, we present FewShotAmazon, the first benchmark for the task of Constrained Text Generation under few-shot settings on multiple domains. Further, we propose the Switch-GPT model, in which we utilize the strong language modeling capacity of GPT-2 to generate fluent and well-formulated sentences, while using a light attention module to decide which constraint to attend to at each step. Experiments show that the proposed Switch-GPT model is effective and remarkably outperforms the baselines. Codes will be available at https://github.com/chang-github-00/Switch-GPT.},
	language = {en},
	number = {11},
	urldate = {2023-10-03},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Ma, Chang and Zhang, Song and Shen, Gehui and Deng, Zhihong},
	month = jun,
	year = {2022},
	note = {Number: 11},
	keywords = {Text Generation},
	pages = {13011--13012},
}

@misc{belouadi_bygpt5_2023,
	title = {{ByGPT5}: {End}-to-{End} {Style}-conditioned {Poetry} {Generation} with {Token}-free {Language} {Models}},
	shorttitle = {{ByGPT5}},
	url = {http://arxiv.org/abs/2212.10474},
	doi = {10.48550/arXiv.2212.10474},
	abstract = {State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Belouadi, Jonas and Eger, Steffen},
	month = may,
	year = {2023},
	note = {arXiv:2212.10474 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{colton_full-face_2012,
	title = {Full-{FACE} {Poetry} {Generation}},
	url = {https://www.semanticscholar.org/paper/Full-FACE-Poetry-Generation-Colton-Goodwin/c07826257067c443f7793ca97161fa48ab7bdc17},
	abstract = {We describe a corpus-based poetry generation system which uses templates to construct poems according to given constraints on rhyme, meter, stress, sentiment, word frequency and word similarity. Moreover, the software constructs a mood for the day by analysing newspaper articles; uses this to determine both an article to base a poem on and a template for the poem; creates an aesthetic based on relevance to the article, lyricism, sentiment and flamboyancy; searches for an instantiation of the template which maximises the aesthetic; and provides a commentary for the whole process to add value to the creative act. We describe the processes behind this approach, present some experimental results which helped infine tuning, and provide some illustrative poems and commentaries. We argue that this is the first poetry system which generates examples, forms concepts, invents aesthetics and frames its work, and so can be assessed favourably with respect to the FACE model for comparing creative systems.},
	urldate = {2023-10-03},
	author = {Colton, S. and Goodwin, J. and Veale, T.},
	year = {2012},
}

@article{gatti_pragmatic_2022,
	title = {Pragmatic evaluations of automated linguistic creativity},
	volume = {56},
	issn = {1574-020X, 1574-0218},
	url = {https://link.springer.com/10.1007/s10579-021-09560-6},
	doi = {10.1007/s10579-021-09560-6},
	abstract = {Abstract
            The optimal innovation hypothesis (OIH) offers a good aesthetic and cognitive reference for the kind of linguistic creativity where minimal variations can have a strong effect on the audience and realize an overall intended goal. The same approach can be the basis for creative systems. The question is how to concretely evaluate not only their quality, but also their pragmatic effect. This paper describes the original evaluations of two systems based on the OIH, one automatically yielding witty headlines for incoming news, the other producing song parodies, varying the song lyrics to evoke a given concept. The goal is to bring attention to the importance of evaluating the pragmatic potential of creative systems, in addition to the quality of their output, and to demonstrate how such evaluations can be done.},
	language = {en},
	number = {2},
	urldate = {2023-10-03},
	journal = {Language Resources and Evaluation},
	author = {Gatti, Lorenzo and Stock, Oliviero and Strapparava, Carlo and Özbal, Gözde},
	month = jun,
	year = {2022},
	pages = {451--476},
}

@inproceedings{gatti_sing_2017,
	address = {Valencia, Spain},
	title = {To {Sing} like a {Mockingbird}},
	url = {http://aclweb.org/anthology/E17-2048},
	doi = {10.18653/v1/E17-2048},
	abstract = {Musical parody, i.e. the act of changing the lyrics of an existing and very well-known song, is a commonly used technique for creating catchy advertising tunes and for mocking people or events. Here we describe a system for automatically producing a musical parody, starting from a corpus of songs. The system can automatically identify characterizing words and concepts related to a novel text, which are taken from the daily news. These concepts are then used as seeds to appropriately replace part of the original lyrics of a song, using metrical, rhyming and lexical constraints. Finally, the parody can be sung with a singing speech synthesizer, with no intervention from the user.},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the           {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Gatti, Lorenzo and Özbal, Gözde and Stock, Oliviero and Strapparava, Carlo},
	year = {2017},
	pages = {298--304},
}

@misc{zhang_pointer_2020,
	title = {{POINTER}: {Constrained} {Progressive} {Text} {Generation} via {Insertion}-based {Generative} {Pre}-training},
	shorttitle = {{POINTER}},
	url = {http://arxiv.org/abs/2005.00558},
	doi = {10.48550/arXiv.2005.00558},
	abstract = {Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation. However, these models cannot be directly employed to generate text under specified lexical constraints. To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation. The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner. This procedure is recursively applied until a sequence is completed. The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable. We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks. Non-autoregressive decoding yields an empirically logarithmic time complexity during inference time. Experimental results on both News and Yelp datasets demonstrate that POINTER achieves state-of-the-art performance on constrained text generation. We released the pre-trained models and the source code to facilitate future research (https://github.com/dreasysnail/POINTER).},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Zhang, Yizhe and Wang, Guoyin and Li, Chunyuan and Gan, Zhe and Brockett, Chris and Dolan, Bill},
	month = sep,
	year = {2020},
	note = {arXiv:2005.00558 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{yang_fudge_2021,
	title = {{FUDGE}: {Controlled} {Text} {Generation} {With} {Future} {Discriminators}},
	shorttitle = {{FUDGE}},
	url = {http://arxiv.org/abs/2104.05218},
	doi = {10.18653/v1/2021.naacl-main.276},
	abstract = {We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in language generation, and formality change in machine translation -- and observe gains in all three tasks.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	author = {Yang, Kevin and Klein, Dan},
	year = {2021},
	note = {arXiv:2104.05218 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {3511--3535},
}

@inproceedings{post_fast_2018,
	address = {New Orleans, Louisiana},
	title = {Fast {Lexically} {Constrained} {Decoding} with {Dynamic} {Beam} {Allocation} for {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/N18-1119},
	doi = {10.18653/v1/N18-1119},
	abstract = {The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm's remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Post, Matt and Vilar, David},
	month = jun,
	year = {2018},
	pages = {1314--1324},
}

@misc{meng_controllable_2022,
	title = {Controllable {Text} {Generation} with {Neurally}-{Decomposed} {Oracle}},
	url = {http://arxiv.org/abs/2205.14219},
	doi = {10.48550/arXiv.2205.14219},
	abstract = {We propose a general and efficient framework to control auto-regressive generation models with NeurAlly-Decomposed Oracle (NADO). Given a pre-trained base language model and a sequence-level boolean oracle function, we propose to decompose the oracle function into token-level guidance to steer the base model in text generation. Specifically, the token-level guidance is approximated by a neural model trained with examples sampled from the base model, demanding no additional auxiliary labeled data. Based on posterior regularization, we present the closed-form optimal solution to incorporate the token-level guidance into the base model for controllable generation. We further provide a theoretical analysis of how the approximation quality of NADO affects the controllable generation results. Experiments conducted on two applications: (1) text generation with lexical constraints and (2) machine translation with formality control demonstrate that our framework efficiently guides the base model towards the given oracle while maintaining high generation quality.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Meng, Tao and Lu, Sidi and Peng, Nanyun and Chang, Kai-Wei},
	month = oct,
	year = {2022},
	note = {arXiv:2205.14219 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lu_neurologic_2021,
	title = {{NeuroLogic} {Decoding}: ({Un})supervised {Neural} {Text} {Generation} with {Predicate} {Logic} {Constraints}},
	shorttitle = {{NeuroLogic} {Decoding}},
	url = {http://arxiv.org/abs/2010.12884},
	doi = {10.48550/arXiv.2010.12884},
	abstract = {Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models -- supervised or not -- to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Lu, Ximing and West, Peter and Zellers, Rowan and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = apr,
	year = {2021},
	note = {arXiv:2010.12884 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lu_insnet_2022,
	title = {{InsNet}: {An} {Efficient}, {Flexible}, and {Performant} {Insertion}-based {Text} {Generation} {Model}},
	shorttitle = {{InsNet}},
	url = {http://arxiv.org/abs/2102.11008},
	doi = {10.48550/arXiv.2102.11008},
	abstract = {We propose InsNet, an expressive insertion-based text generator with efficient training and flexible decoding (parallel or sequential). Unlike most existing insertion-based text generation works that require re-encoding of the context after each insertion operation and thus are inefficient to train, InsNet only requires one pass of context encoding for the entire sequence during training by introducing a novel insertion-oriented position encoding and a light-weighted slot representation strategy to enable computation sharing. Furthermore, we propose an algorithm InsNet-Dinic to better determine the parallelization of insertion operations that provides a controllable switch between parallel and sequential decoding, making it flexible to handle more parallelizable tasks such as machine translation with efficient decoding, or less parallelizable tasks such as open-domain text generation to guarantee high-quality outputs. Experiments on two lexically constrained text generation datasets and three machine translation datasets demonstrate InsNet's advantages over previous insertion-based methods in terms of training speed, inference efficiency, and generation quality.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Lu, Sidi and Meng, Tao and Peng, Nanyun},
	month = oct,
	year = {2022},
	note = {arXiv:2102.11008 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{lin_commongen_2020,
	title = {{CommonGen}: {A} {Constrained} {Text} {Generation} {Challenge} for {Generative} {Commonsense} {Reasoning}},
	shorttitle = {{CommonGen}},
	url = {http://arxiv.org/abs/1911.03705},
	doi = {10.48550/arXiv.1911.03705},
	abstract = {Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., \{dog, frisbee, catch, throw\}); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., "a man throws a frisbee and his dog catches it"). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge, and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 79k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance. Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA by generating additional context.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Lin, Bill Yuchen and Zhou, Wangchunshu and Shen, Ming and Zhou, Pei and Bhagavatula, Chandra and Choi, Yejin and Ren, Xiang},
	month = nov,
	year = {2020},
	note = {arXiv:1911.03705 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{dang_tractable_2023,
	title = {Tractable and {Expressive} {Generative} {Models} of {Genetic} {Variation} {Data}},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.05.16.541036v1},
	doi = {10.1101/2023.05.16.541036},
	abstract = {Population genetic studies often rely on artificial genomes (AGs) simulated by generative models of genetic data. In recent years, unsupervised learning models, based on hidden Markov models, deep generative adversarial networks, restricted Boltzmann machines, and variational autoencoders, have gained popularity due to their ability to generate AGs closely resembling empirical data. These models, however, present a tradeoff between expressivity and tractability. Here, we propose to use hidden Chow-Liu trees (HCLTs) and their representation as probabilistic circuits (PCs) as a solution to this tradeoff. We first learn an HCLT structure that captures the long-range dependencies among SNPs in the training data set. We then convert the HCLT to its equivalent PC as a means of supporting tractable and efficient probabilistic inference. The parameters in these PCs are inferred with an expectation-maximization algorithm using the training data. Compared to other models for generating AGs, HCLT obtains the largest log-likelihood on test genomes across SNPs chosen across the genome and from a contiguous genomic region. Moreover, the AGs generated by HCLT more accurately resemble the source data set in their patterns of allele frequencies, linkage disequilibrium, pairwise haplotype distances, and population structure. This work not only presents a new and robust AG simulator but also manifests the potential of PCs in population genetics.},
	language = {en},
	urldate = {2023-10-03},
	publisher = {bioRxiv},
	author = {Dang, Meihua and Liu, Anji and Wei, Xinzhu and Sankararaman, Sriram and Broeck, Guy Van den},
	month = may,
	year = {2023},
	note = {Pages: 2023.05.16.541036
Section: New Results},
}

@misc{yang_xlnet_2020,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {http://arxiv.org/abs/1906.08237},
	doi = {10.48550/arXiv.1906.08237},
	abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jan,
	year = {2020},
	note = {arXiv:1906.08237 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{joshi_bringing_2017,
	title = {Bringing {Impressionism} to {Life} with {Neural} {Style} {Transfer} in {Come} {Swim}},
	url = {http://arxiv.org/abs/1701.04928},
	doi = {10.48550/arXiv.1701.04928},
	abstract = {Neural Style Transfer is a striking, recently-developed technique that uses neural networks to artistically redraw an image in the style of a source style image. This paper explores the use of this technique in a production setting, applying Neural Style Transfer to redraw key scenes in 'Come Swim' in the style of the impressionistic painting that inspired the film. We document how the technique can be driven within the framework of an iterative creative process to achieve a desired look, and propose a mapping of the broad parameter space to a key set of creative controls. We hope that this mapping can provide insights into priorities for future research.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Joshi, Bhautik and Stewart, Kristen and Shapiro, David},
	month = jan,
	year = {2017},
	note = {arXiv:1701.04928 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, I.3.3, I.4.0},
}

@inproceedings{ghazvininejad_generating_2016,
	address = {Austin, Texas},
	title = {Generating {Topical} {Poetry}},
	url = {https://aclanthology.org/D16-1126},
	doi = {10.18653/v1/D16-1126},
	urldate = {2023-10-03},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ghazvininejad, Marjan and Shi, Xing and Choi, Yejin and Knight, Kevin},
	month = nov,
	year = {2016},
	pages = {1183--1191},
}

@misc{riedl_weird_2020,
	title = {Weird {AI} {Yankovic}: {Generating} {Parody} {Lyrics}},
	shorttitle = {Weird {AI} {Yankovic}},
	url = {http://arxiv.org/abs/2009.12240},
	doi = {10.48550/arXiv.2009.12240},
	abstract = {Lyrics parody swaps one set of words that accompany a melody with a new set of words, preserving the number of syllables per line and the rhyme scheme. Lyrics parody generation is a challenge for controllable text generation. We show how a specialized sampling procedure, combined with backward text generation with XLNet can produce parody lyrics that reliably meet the syllable and rhyme scheme constraints.We introduce the Weird AI Yankovic system and provide a case study evaluation. We conclude with societal implications of neural lyric parody generation.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Riedl, Mark},
	month = sep,
	year = {2020},
	note = {arXiv:2009.12240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2023-10-02},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zhang_tractable_2023,
	title = {Tractable {Control} for {Autoregressive} {Language} {Generation}},
	url = {http://arxiv.org/abs/2304.07438},
	doi = {10.48550/arXiv.2304.07438},
	abstract = {Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution \$\{{\textbackslash}Pr\}({\textbackslash}text\{text\} {\textbar} {\textbackslash}alpha)\$ is intractable for even the simplest lexical constraints \${\textbackslash}alpha\$. To overcome this challenge, we propose to use tractable probabilistic models (TPMs) to impose lexical constraints in autoregressive text generation models, which we refer to as GeLaTo (Generating Language with Tractable Constraints). To demonstrate the effectiveness of this framework, we use distilled hidden Markov models, where we can efficiently compute \$\{{\textbackslash}Pr\}({\textbackslash}text\{text\} {\textbar} {\textbackslash}alpha)\$, to guide autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on challenging benchmarks for constrained text generation (e.g., CommonGen), beating various strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive TPMs.},
	urldate = {2023-09-30},
	publisher = {arXiv},
	author = {Zhang, Honghua and Dang, Meihua and Peng, Nanyun and Broeck, Guy Van den},
	month = jun,
	year = {2023},
	note = {arXiv:2304.07438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
